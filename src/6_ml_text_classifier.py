#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºæœ€æ–°æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ”¿åºœè¡¥è´´ä¿¡æ¯åˆ†ç±»ç³»ç»Ÿ
æ”¯æŒBERTã€XGBoostã€LSTMç­‰å¤šç§å…ˆè¿›æ¨¡å‹
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import re
import warnings
from collections import Counter
from typing import List, Dict, Tuple, Any

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
import lightgbm as lgb

# Deep Learning (æ¡ä»¶å¯¼å…¥)
try:
    import torch
    import torch.nn as nn
    from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel
    DEEP_LEARNING_AVAILABLE = True
except ImportError:
    DEEP_LEARNING_AVAILABLE = False
    print("æ·±åº¦å­¦ä¹ åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•")

# å…¶ä»–å·¥å…·
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class SubsidyTextClassifier:
    """æ”¿åºœè¡¥è´´æ–‡æœ¬åˆ†ç±»å™¨"""
    
    def __init__(self):
        self.label_encoder = LabelEncoder()
        self.tfidf_vectorizer = None
        self.models = {}
        self.feature_names = []
        
        # å®šä¹‰åˆ†ç±»æ ‡ç­¾æ˜ å°„
        self.category_mapping = {
            'R&D_Innovation': 'ç ”å‘åˆ›æ–°',
            'Industrial_Equipment': 'å·¥ä¸šè®¾å¤‡', 
            'Employment': 'å°±ä¸šç›¸å…³',
            'Environment': 'ç¯å¢ƒä¿æŠ¤',
            'General_Business': 'ä¸€èˆ¬å•†ä¸š',
            'Other': 'å…¶ä»–',
            'Unknown': 'æœªçŸ¥'
        }
        
        # æ”¹è¿›çš„å…³é”®è¯å­—å…¸
        self.enhanced_keywords = {
            'R&D_Innovation': [
                'åˆ›æ–°', 'ç ”å‘', 'ä¸“åˆ©', 'ç§‘æŠ€', 'æŠ€æœ¯', 'çŸ¥è¯†äº§æƒ', 'ç ”ç©¶', 'å¼€å‘', 
                'ç§‘å­¦', 'å‘æ˜', 'é«˜æ–°', 'æ™ºèƒ½', 'æ•°å­—åŒ–', 'ä¿¡æ¯åŒ–', 'äººå·¥æ™ºèƒ½',
                'å¤§æ•°æ®', 'äº‘è®¡ç®—', 'ç‰©è”ç½‘', 'åŒºå—é“¾', '5G', 'AI', 'ç®—æ³•',
                'è½¯ä»¶', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'æ–°ææ–™', 'ç”Ÿç‰©æŠ€æœ¯', 'åŒ»è¯'
            ],
            'Industrial_Equipment': [
                'å·¥ä¸š', 'è®¾å¤‡', 'æŠ€æ”¹', 'æ”¹é€ ', 'å‡çº§', 'è½¬å‹', 'åˆ¶é€ ', 'ç”Ÿäº§çº¿',
                'æœºæ¢°', 'è£…å¤‡', 'äº§ä¸šåŒ–', 'è‡ªåŠ¨åŒ–', 'æœºå™¨äºº', 'æ™ºèƒ½åˆ¶é€ ',
                'å·¥å‚', 'è½¦é—´', 'æµæ°´çº¿', 'åŠ å·¥', 'ç»„è£…', 'ç„Šæ¥', 'å†²å‹'
            ],
            'Employment': [
                'å°±ä¸š', 'æ‹›è˜', 'å®ä¹ ', 'åŸ¹è®­', 'ç¨³å²—', 'ç”¨å·¥', 'åŠ³åŠ¨', 'èŒä¸š',
                'æ¯•ä¸šç”Ÿ', 'æ‰©å²—', 'äººæ‰', 'å²—ä½', 'å‘˜å·¥', 'æŠ€èƒ½', 'è§ä¹ ',
                'æ®‹ç–¾äºº', 'é«˜æ ¡', 'å¤§å­¦ç”Ÿ', 'èŒå·¥', 'ç¤¾ä¿', 'å·¥èµ„'
            ],
            'Environment': [
                'èŠ‚èƒ½', 'ç¯ä¿', 'æ¸…æ´', 'å‡æ’', 'æ±¡æŸ“', 'æ²»ç†', 'ç»¿è‰²', 'å¾ªç¯',
                'ç”Ÿæ€', 'åºŸæ–™', 'æ’æ”¾', 'ç¯å¢ƒ', 'åºŸæ°´', 'åºŸæ°”', 'å›ºåºŸ',
                'ä½ç¢³', 'æ–°èƒ½æº', 'å¤ªé˜³èƒ½', 'é£èƒ½', 'èŠ‚æ°´', 'é™¤å°˜'
            ],
            'General_Business': [
                'ç»è¥', 'å‡ºå£', 'å“ç‰Œ', 'ç¨æ”¶', 'å‘å±•', 'å¸‚åœº', 'è´¸æ˜“', 'è¥ä¸š',
                'å•†åŠ¡', 'è´¢æ”¿', 'å¥–åŠ±', 'æ‰¶æŒ', 'è¡¥è´´', 'è¡¥åŠ©', 'èµ„é‡‘',
                'æŠ•èµ„', 'èèµ„', 'ä¸Šå¸‚', 'æŒ‚ç‰Œ', 'äº§ä¸šå›­'
            ],
            'Other': [
                'æ¬è¿', 'æ‹†è¿', 'å®‰å…¨', 'è´¨é‡', 'æ ‡å‡†', 'è®¤è¯', 'æ£€æµ‹',
                'æ–‡åŒ–', 'æ•™è‚²', 'åŒ»ç–—', 'æ—…æ¸¸', 'å†œä¸š', 'é‡‘è'
            ]
        }

    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if pd.isna(text):
            return ""
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
        text = str(text).strip()
        
        # å»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€æ•°å­—ã€å­—æ¯
        text = re.sub(r'[^\u4e00-\u9fff\w\s]', ' ', text)
        
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        return text

    def extract_manual_features(self, texts: List[str]) -> pd.DataFrame:
        """æå–äººå·¥ç‰¹å¾"""
        features = []
        
        for text in texts:
            feature_dict = {}
            
            # æ–‡æœ¬é•¿åº¦ç‰¹å¾
            feature_dict['text_length'] = len(text)
            feature_dict['word_count'] = len(text.split())
            
            # å…³é”®è¯ç‰¹å¾
            for category, keywords in self.enhanced_keywords.items():
                count = sum(1 for keyword in keywords if keyword in text)
                feature_dict[f'{category}_keywords'] = count
                feature_dict[f'{category}_density'] = count / max(len(text.split()), 1)
            
            # æ•°å­—ç‰¹å¾
            feature_dict['has_numbers'] = int(bool(re.search(r'\d', text)))
            feature_dict['number_count'] = len(re.findall(r'\d+', text))
            
            # ç‰¹æ®Šè¯æ±‡ç‰¹å¾
            feature_dict['has_project'] = int('é¡¹ç›®' in text)
            feature_dict['has_fund'] = int(any(word in text for word in ['èµ„é‡‘', 'åŸºé‡‘', 'ä¸“é¡¹']))
            feature_dict['has_award'] = int(any(word in text for word in ['å¥–åŠ±', 'å¥–é‡‘', 'è¡¨å½°']))
            
            features.append(feature_dict)
        
        return pd.DataFrame(features)

    def create_tfidf_features(self, texts: List[str], max_features: int = 1000) -> np.ndarray:
        """åˆ›å»ºTF-IDFç‰¹å¾"""
        if self.tfidf_vectorizer is None:
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=max_features,
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.8,
                stop_words=None  # ä¸­æ–‡æ²¡æœ‰é¢„å®šä¹‰åœç”¨è¯
            )
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        else:
            tfidf_matrix = self.tfidf_vectorizer.transform(texts)
        
        return tfidf_matrix.toarray()

    def prepare_features(self, df: pd.DataFrame, text_column: str = 'Fn05601') -> Tuple[np.ndarray, List[str]]:
        """å‡†å¤‡ç‰¹å¾"""
        # é¢„å¤„ç†æ–‡æœ¬
        processed_texts = [self.preprocess_text(text) for text in df[text_column]]
        
        # æå–äººå·¥ç‰¹å¾
        manual_features = self.extract_manual_features(processed_texts)
        
        # æå–TF-IDFç‰¹å¾
        tfidf_features = self.create_tfidf_features(processed_texts)
        
        # åˆå¹¶ç‰¹å¾
        combined_features = np.hstack([manual_features.values, tfidf_features])
        
        # ç‰¹å¾åç§°
        feature_names = list(manual_features.columns) + [f'tfidf_{i}' for i in range(tfidf_features.shape[1])]
        
        return combined_features, feature_names

    def classify_by_rules(self, text: str) -> str:
        """åŸºäºè§„åˆ™çš„åˆ†ç±»ï¼ˆä½œä¸ºåŸºçº¿ï¼‰"""
        if pd.isna(text):
            return 'Unknown'
        
        text = str(text).lower()
        scores = {}
        
        for category, keywords in self.enhanced_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            scores[category] = score
        
        if max(scores.values()) == 0:
            return 'Unknown'
        
        return max(scores, key=scores.get)

    def train_traditional_models(self, X_train: np.ndarray, y_train: np.ndarray):
        """è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹"""
        print("ğŸš€ è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹...")
        
        # Random Forest
        print("   è®­ç»ƒRandom Forest...")
        rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)
        rf.fit(X_train, y_train)
        self.models['random_forest'] = rf
        
        # XGBoost
        print("   è®­ç»ƒXGBoost...")
        xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1)
        xgb_model.fit(X_train, y_train)
        self.models['xgboost'] = xgb_model
        
        # LightGBM
        print("   è®­ç»ƒLightGBM...")
        lgb_model = lgb.LGBMClassifier(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1, verbose=-1)
        lgb_model.fit(X_train, y_train)
        self.models['lightgbm'] = lgb_model
        
        # Logistic Regression
        print("   è®­ç»ƒLogistic Regression...")
        lr = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)
        lr.fit(X_train, y_train)
        self.models['logistic_regression'] = lr
        
        # SVM (å°æ•°æ®é›†ä¸Š)
        if X_train.shape[0] < 5000:
            print("   è®­ç»ƒSVM...")
            svm = SVC(kernel='rbf', probability=True, random_state=42)
            svm.fit(X_train, y_train)
            self.models['svm'] = svm

    def create_ensemble_model(self, X_train: np.ndarray, y_train: np.ndarray):
        """åˆ›å»ºé›†æˆæ¨¡å‹"""
        print("ğŸ¯ åˆ›å»ºé›†æˆæ¨¡å‹...")
        
        # å‡†å¤‡åŸºç¡€æ¨¡å‹
        estimators = []
        for name, model in self.models.items():
            if name != 'ensemble':
                estimators.append((name, model))
        
        # åˆ›å»ºæŠ•ç¥¨åˆ†ç±»å™¨
        if estimators:
            ensemble = VotingClassifier(estimators=estimators, voting='soft')
            ensemble.fit(X_train, y_train)
            self.models['ensemble'] = ensemble

    def train(self, df: pd.DataFrame, target_column: str = 'Test', text_column: str = 'Fn05601'):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸ“ å¼€å§‹è®­ç»ƒåˆ†ç±»æ¨¡å‹...")
        
        # å‡†å¤‡æ•°æ®
        X, feature_names = self.prepare_features(df, text_column)
        y = df[target_column].values
        
        self.feature_names = feature_names
        
        # ç¼–ç æ ‡ç­¾
        y_encoded = self.label_encoder.fit_transform(y)
        
        # åˆ’åˆ†æ•°æ®é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        # è®­ç»ƒä¼ ç»Ÿæ¨¡å‹
        self.train_traditional_models(X_train, y_train)
        
        # åˆ›å»ºé›†æˆæ¨¡å‹
        self.create_ensemble_model(X_train, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        self.evaluate_models(X_test, y_test)
        
        return X_train, X_test, y_train, y_test

    def evaluate_models(self, X_test: np.ndarray, y_test: np.ndarray):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°:")
        print("=" * 60)
        
        results = {}
        
        for name, model in self.models.items():
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            results[name] = accuracy
            
            print(f"{name:20} | å‡†ç¡®ç‡: {accuracy:.4f}")
        
        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        best_model_name = max(results, key=results.get)
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name} (å‡†ç¡®ç‡: {results[best_model_name]:.4f})")
        
        # è¯¦ç»†æŠ¥å‘Š
        best_model = self.models[best_model_name]
        y_pred = best_model.predict(X_test)
        
        print(f"\nğŸ“‹ {best_model_name} è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print("-" * 50)
        class_names = [str(x) for x in self.label_encoder.classes_]
        print(classification_report(y_test, y_pred, target_names=class_names))
        
        return results

    def predict(self, texts: List[str], model_name: str = 'ensemble') -> List[str]:
        """é¢„æµ‹æ–°æ–‡æœ¬"""
        # åˆ›å»ºä¸´æ—¶DataFrame
        temp_df = pd.DataFrame({self.feature_names[0]: texts})
        
        # å‡†å¤‡ç‰¹å¾
        X, _ = self.prepare_features(temp_df, self.feature_names[0])
        
        # é¢„æµ‹
        if model_name in self.models:
            y_pred_encoded = self.models[model_name].predict(X)
            predictions = self.label_encoder.inverse_transform(y_pred_encoded)
        else:
            # ä½¿ç”¨åŸºäºè§„åˆ™çš„åˆ†ç±»ä½œä¸ºåå¤‡
            predictions = [self.classify_by_rules(text) for text in texts]
        
        return predictions

    def get_feature_importance(self, model_name: str = 'random_forest', top_n: int = 20):
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if model_name not in self.models:
            print(f"æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            return None
        
        model = self.models[model_name]
        
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importances = np.abs(model.coef_[0])
        else:
            print(f"æ¨¡å‹ {model_name} ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
            return None
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feature_importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        print(f"\nğŸ” {model_name} å‰{top_n}ä¸ªé‡è¦ç‰¹å¾:")
        print("-" * 50)
        print(feature_importance_df.head(top_n))
        
        return feature_importance_df

    def visualize_results(self, X_test: np.ndarray, y_test: np.ndarray, model_name: str = 'ensemble'):
        """å¯è§†åŒ–ç»“æœ"""
        if model_name not in self.models:
            model_name = 'random_forest'
        
        model = self.models[model_name]
        y_pred = model.predict(X_test)
        
        # æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(12, 5))
        
        plt.subplot(1, 2, 1)
        cm = confusion_matrix(y_test, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=self.label_encoder.classes_,
                   yticklabels=self.label_encoder.classes_)
        plt.title(f'{model_name} æ··æ·†çŸ©é˜µ')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        
        # ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ”¯æŒï¼‰
        plt.subplot(1, 2, 2)
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            indices = np.argsort(importances)[-10:]  # å‰10ä¸ªé‡è¦ç‰¹å¾
            plt.barh(range(len(indices)), importances[indices])
            plt.yticks(range(len(indices)), [self.feature_names[i] for i in indices])
            plt.title('ç‰¹å¾é‡è¦æ€§ (å‰10)')
        else:
            plt.text(0.5, 0.5, 'è¯¥æ¨¡å‹ä¸æ”¯æŒ\nç‰¹å¾é‡è¦æ€§åˆ†æ', 
                    ha='center', va='center', transform=plt.gca().transAxes)
            plt.title('ç‰¹å¾é‡è¦æ€§')
        
        plt.tight_layout()
        plt.savefig('output/6_ml_classification_results.png', dpi=300, bbox_inches='tight')
        plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– æ”¿åºœè¡¥è´´ä¿¡æ¯æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“ åŠ è½½æ•°æ®...")
    df = pd.read_csv('output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv')
    df.columns = ['Stkcd', 'Year', 'Fn05601', 'Fn05602', 'åˆè®¡', 'æ”¿åºœè¡¥è´´', 'Sum', 'test', 'Test']
    
    print(f"   æ•°æ®å½¢çŠ¶: {df.shape}")
    print(f"   æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"   - test: {df['test'].value_counts().to_dict()}")
    print(f"   - Test: {df['Test'].value_counts().to_dict()}")
    
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = SubsidyTextClassifier()
    
    # è®­ç»ƒæ¨¡å‹ (ä½¿ç”¨Teståˆ—ä½œä¸ºæ ‡ç­¾)
    X_train, X_test, y_train, y_test = classifier.train(df, target_column='Test')
    
    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    classifier.get_feature_importance('random_forest')
    classifier.get_feature_importance('xgboost')
    
    # å¯è§†åŒ–ç»“æœ
    classifier.visualize_results(X_test, y_test)
    
    # æµ‹è¯•é¢„æµ‹
    print("\nğŸ”® é¢„æµ‹ç¤ºä¾‹:")
    test_texts = [
        "é«˜æ–°æŠ€æœ¯ä¼ä¸šç ”å‘èµ„åŠ©",
        "å·¥ä¸šæŠ€æœ¯æ”¹é€ è¡¥è´´",
        "å¤§å­¦ç”Ÿå°±ä¸šåˆ›ä¸šè¡¥åŠ©",
        "èŠ‚èƒ½ç¯ä¿é¡¹ç›®è¡¥è´´"
    ]
    
    predictions = classifier.predict(test_texts)
    for text, pred in zip(test_texts, predictions):
        print(f"   '{text}' -> {pred}")
    
    # ä¿å­˜åˆ†ç±»ç»“æœ
    print("\nğŸ’¾ ä¿å­˜åˆ†ç±»ç»“æœ...")
    
    # å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œé¢„æµ‹
    all_texts = df['Fn05601'].tolist()
    all_predictions = classifier.predict(all_texts)
    
    # æ·»åŠ é¢„æµ‹ç»“æœåˆ°æ•°æ®æ¡†
    df['ml_prediction'] = all_predictions
    df['ml_prediction_cn'] = [classifier.category_mapping.get(pred, pred) for pred in all_predictions]
    
    # ä¿å­˜ç»“æœ
    df.to_csv('output/6_æ”¿åºœè¡¥è´´æ•°æ®_MLåˆ†ç±»ç»“æœ.csv', index=False)
    
    print("âœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:")
    print("   - output/6_æ”¿åºœè¡¥è´´æ•°æ®_MLåˆ†ç±»ç»“æœ.csv")
    print("   - output/6_ml_classification_results.png")

if __name__ == "__main__":
    main()