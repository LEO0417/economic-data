#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºBERTå’Œæœ€æ–°æœºå™¨å­¦ä¹ æ–¹æ³•çš„æ”¿åºœè¡¥è´´ä¿¡æ¯æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ
æ•´åˆæ·±åº¦å­¦ä¹ ã€ä¼ ç»ŸMLå’Œè§„åˆ™æ–¹æ³•çš„é›†æˆåˆ†ç±»å™¨
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import jieba
import re
import warnings
import json
from collections import Counter
from typing import List, Dict, Tuple, Any, Optional

# åŸºç¡€ç§‘å­¦è®¡ç®—
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class AdvancedSubsidyClassifier:
    """
    å…ˆè¿›çš„æ”¿åºœè¡¥è´´æ–‡æœ¬æ™ºèƒ½åˆ†ç±»å™¨
    æ•´åˆBERTã€ä¼ ç»ŸMLå’Œè§„åˆ™çš„å¤šå±‚æ¬¡åˆ†ç±»ç³»ç»Ÿ
    """
    
    def __init__(self):
        self.category_definitions = {
            'R&D_Innovation': {
                'name': 'ç ”å‘åˆ›æ–°ç±»',
                'description': 'æ”¯æŒç ”å‘ã€åˆ›æ–°ã€ä¸“åˆ©ã€ç§‘æŠ€ç­‰æ´»åŠ¨',
                'keywords': [
                    'åˆ›æ–°', 'ç ”å‘', 'ä¸“åˆ©', 'ç§‘æŠ€', 'æŠ€æœ¯', 'çŸ¥è¯†äº§æƒ', 'ç ”ç©¶', 'å¼€å‘',
                    'ç§‘å­¦', 'å‘æ˜', 'é«˜æ–°', 'æ™ºèƒ½', 'æ•°å­—åŒ–', 'ä¿¡æ¯åŒ–', 'äººå·¥æ™ºèƒ½',
                    'å¤§æ•°æ®', 'äº‘è®¡ç®—', 'ç‰©è”ç½‘', 'åŒºå—é“¾', '5G', 'AI', 'ç®—æ³•',
                    'è½¯ä»¶', 'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'æ–°ææ–™', 'ç”Ÿç‰©æŠ€æœ¯', 'åŒ»è¯',
                    'ç§‘ç ”', 'å­¦æœ¯', 'æŠ€æœ¯è½¬åŒ–', 'äº§å­¦ç ”', 'å­µåŒ–å™¨'
                ],
                'negative_keywords': ['é”€å”®', 'ç”Ÿäº§', 'åˆ¶é€ ']
            },
            'Industrial_Equipment': {
                'name': 'å·¥ä¸šè®¾å¤‡ç±»',
                'description': 'æ”¯æŒå·¥ä¸šè®¾å¤‡è´­ç½®ã€æŠ€æœ¯æ”¹é€ ã€ç”Ÿäº§çº¿å‡çº§',
                'keywords': [
                    'å·¥ä¸š', 'è®¾å¤‡', 'æŠ€æ”¹', 'æ”¹é€ ', 'å‡çº§', 'è½¬å‹', 'åˆ¶é€ ', 'ç”Ÿäº§çº¿',
                    'æœºæ¢°', 'è£…å¤‡', 'äº§ä¸šåŒ–', 'è‡ªåŠ¨åŒ–', 'æœºå™¨äºº', 'æ™ºèƒ½åˆ¶é€ ',
                    'å·¥å‚', 'è½¦é—´', 'æµæ°´çº¿', 'åŠ å·¥', 'ç»„è£…', 'ç„Šæ¥', 'å†²å‹',
                    'ç”Ÿäº§', 'åˆ¶é€ ä¸š', 'åŠ å·¥ä¸š', 'é‡å·¥ä¸š', 'è½»å·¥ä¸š'
                ],
                'negative_keywords': ['ç ”å‘', 'ä¸“åˆ©', 'å°±ä¸š']
            },
            'Employment': {
                'name': 'å°±ä¸šä¿ƒè¿›ç±»',
                'description': 'æ”¯æŒå°±ä¸šç¨³å®šã€åŸ¹è®­ã€äººæ‰å¼•è¿›ç­‰',
                'keywords': [
                    'å°±ä¸š', 'æ‹›è˜', 'å®ä¹ ', 'åŸ¹è®­', 'ç¨³å²—', 'ç”¨å·¥', 'åŠ³åŠ¨', 'èŒä¸š',
                    'æ¯•ä¸šç”Ÿ', 'æ‰©å²—', 'äººæ‰', 'å²—ä½', 'å‘˜å·¥', 'æŠ€èƒ½', 'è§ä¹ ',
                    'æ®‹ç–¾äºº', 'é«˜æ ¡', 'å¤§å­¦ç”Ÿ', 'èŒå·¥', 'ç¤¾ä¿', 'å·¥èµ„',
                    'äººåŠ›èµ„æº', 'æ‹›å·¥', 'å½•ç”¨', 'è˜ç”¨', 'ç¤¾ä¼šä¿é™©'
                ],
                'negative_keywords': ['è®¾å¤‡', 'æŠ€æœ¯', 'ç¯ä¿']
            },
            'Environment': {
                'name': 'ç¯å¢ƒä¿æŠ¤ç±»',
                'description': 'æ”¯æŒèŠ‚èƒ½ç¯ä¿ã€æ¸…æ´ç”Ÿäº§ã€æ±¡æŸ“æ²»ç†',
                'keywords': [
                    'èŠ‚èƒ½', 'ç¯ä¿', 'æ¸…æ´', 'å‡æ’', 'æ±¡æŸ“', 'æ²»ç†', 'ç»¿è‰²', 'å¾ªç¯',
                    'ç”Ÿæ€', 'åºŸæ–™', 'æ’æ”¾', 'ç¯å¢ƒ', 'åºŸæ°´', 'åºŸæ°”', 'å›ºåºŸ',
                    'ä½ç¢³', 'æ–°èƒ½æº', 'å¤ªé˜³èƒ½', 'é£èƒ½', 'èŠ‚æ°´', 'é™¤å°˜',
                    'ç¯å¢ƒä¿æŠ¤', 'æ¸…æ´èƒ½æº', 'å¯å†ç”Ÿèƒ½æº', 'ç¢³å‡æ’'
                ],
                'negative_keywords': ['å°±ä¸š', 'ç ”å‘', 'å·¥ä¸š']
            },
            'General_Business': {
                'name': 'ä¸€èˆ¬å•†ä¸šç±»',
                'description': 'æ”¯æŒä¸€èˆ¬å•†ä¸šæ´»åŠ¨ã€å¸‚åœºå¼€æ‹“ã€ç»è¥å‘å±•',
                'keywords': [
                    'ç»è¥', 'å‡ºå£', 'å“ç‰Œ', 'ç¨æ”¶', 'å‘å±•', 'å¸‚åœº', 'è´¸æ˜“', 'è¥ä¸š',
                    'å•†åŠ¡', 'è´¢æ”¿', 'å¥–åŠ±', 'æ‰¶æŒ', 'è¡¥è´´', 'è¡¥åŠ©', 'èµ„é‡‘',
                    'æŠ•èµ„', 'èèµ„', 'ä¸Šå¸‚', 'æŒ‚ç‰Œ', 'äº§ä¸šå›­', 'å¼€æ‹“',
                    'é”€å”®', 'æ¨å¹¿', 'å®£ä¼ ', 'å±•è§ˆ', 'åšè§ˆä¼š'
                ],
                'negative_keywords': []
            },
            'Other': {
                'name': 'å…¶ä»–ç±»åˆ«',
                'description': 'å…¶ä»–ç‰¹å®šç”¨é€”çš„è¡¥è´´',
                'keywords': [
                    'æ¬è¿', 'æ‹†è¿', 'å®‰å…¨', 'è´¨é‡', 'æ ‡å‡†', 'è®¤è¯', 'æ£€æµ‹',
                    'æ–‡åŒ–', 'æ•™è‚²', 'åŒ»ç–—', 'æ—…æ¸¸', 'å†œä¸š', 'é‡‘è', 'ä¿é™©'
                ],
                'negative_keywords': []
            }
        }
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.text_preprocessor = TextPreprocessor()
        self.feature_extractor = FeatureExtractor(self.category_definitions)
        self.ensemble_classifier = EnsembleClassifier()
        
    def classify_text(self, text: str) -> Dict[str, Any]:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œåˆ†ç±»
        è¿”å›è¯¦ç»†çš„åˆ†ç±»ç»“æœåŒ…æ‹¬æ¦‚ç‡ã€ç½®ä¿¡åº¦ç­‰
        """
        # é¢„å¤„ç†
        processed_text = self.text_preprocessor.process(text)
        
        # ç‰¹å¾æå–
        features = self.feature_extractor.extract(processed_text)
        
        # å¤šæ–¹æ³•åˆ†ç±»
        results = {}
        
        # 1. åŸºäºè§„åˆ™çš„åˆ†ç±»
        rule_result = self._classify_by_rules(processed_text)
        results['rule_based'] = rule_result
        
        # 2. åŸºäºå…³é”®è¯å¯†åº¦çš„åˆ†ç±»
        density_result = self._classify_by_density(processed_text)
        results['density_based'] = density_result
        
        # 3. åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ†ç±»
        semantic_result = self._classify_by_semantic(processed_text)
        results['semantic_based'] = semantic_result
        
        # 4. é›†æˆå†³ç­–
        final_result = self._ensemble_decision(results, features)
        
        return {
            'text': text,
            'processed_text': processed_text,
            'prediction': final_result['category'],
            'confidence': final_result['confidence'],
            'probability_distribution': final_result['probabilities'],
            'features': features,
            'sub_results': results,
            'explanation': self._generate_explanation(final_result, results)
        }
    
    def _classify_by_rules(self, text: str) -> Dict[str, Any]:
        """åŸºäºè§„åˆ™çš„åˆ†ç±»"""
        scores = {}
        
        for category, info in self.category_definitions.items():
            # æ­£å‘å…³é”®è¯å¾—åˆ†
            positive_score = sum(1 for keyword in info['keywords'] if keyword in text)
            
            # è´Ÿå‘å…³é”®è¯æƒ©ç½š
            negative_score = sum(1 for keyword in info.get('negative_keywords', []) if keyword in text)
            
            # è®¡ç®—æœ€ç»ˆå¾—åˆ†
            final_score = positive_score - 0.5 * negative_score
            scores[category] = max(0, final_score)
        
        # å½’ä¸€åŒ–
        total = sum(scores.values())
        if total == 0:
            return {'category': 'Unknown', 'confidence': 0.0, 'scores': scores}
        
        probabilities = {k: v/total for k, v in scores.items()}
        best_category = max(probabilities, key=probabilities.get)
        
        return {
            'category': best_category,
            'confidence': probabilities[best_category],
            'scores': scores,
            'probabilities': probabilities
        }
    
    def _classify_by_density(self, text: str) -> Dict[str, Any]:
        """åŸºäºå…³é”®è¯å¯†åº¦çš„åˆ†ç±»"""
        text_length = len(text.split())
        if text_length == 0:
            return {'category': 'Unknown', 'confidence': 0.0}
        
        densities = {}
        
        for category, info in self.category_definitions.items():
            keyword_count = sum(1 for keyword in info['keywords'] if keyword in text)
            density = keyword_count / text_length
            densities[category] = density
        
        best_category = max(densities, key=densities.get)
        max_density = densities[best_category]
        
        return {
            'category': best_category if max_density > 0 else 'Unknown',
            'confidence': min(max_density * 10, 1.0),  # è°ƒæ•´ç½®ä¿¡åº¦èŒƒå›´
            'densities': densities
        }
    
    def _classify_by_semantic(self, text: str) -> Dict[str, Any]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„åˆ†ç±»ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œå®ç°ä¸€ä¸ªç®€åŒ–çš„è¯­ä¹‰åˆ†ç±»
        # åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨BERT embeddings
        
        semantic_scores = {}
        
        for category, info in self.category_definitions.items():
            # è®¡ç®—ä¸ç±»åˆ«æè¿°çš„ç›¸ä¼¼åº¦
            description_words = set(info['description'].split())
            text_words = set(text.split())
            
            # ç®€å•çš„Jaccardç›¸ä¼¼åº¦
            intersection = len(description_words & text_words)
            union = len(description_words | text_words)
            
            similarity = intersection / union if union > 0 else 0
            semantic_scores[category] = similarity
        
        best_category = max(semantic_scores, key=semantic_scores.get)
        
        return {
            'category': best_category,
            'confidence': semantic_scores[best_category],
            'similarities': semantic_scores
        }
    
    def _ensemble_decision(self, results: Dict, features: Dict) -> Dict[str, Any]:
        """é›†æˆå¤šç§æ–¹æ³•çš„å†³ç­–"""
        # æ”¶é›†æ‰€æœ‰é¢„æµ‹
        predictions = [results['rule_based'], results['density_based'], results['semantic_based']]
        
        # åŠ æƒæŠ•ç¥¨
        weights = {'rule_based': 0.4, 'density_based': 0.3, 'semantic_based': 0.3}
        
        category_votes = {}
        
        for method, result in results.items():
            category = result['category']
            confidence = result['confidence']
            weight = weights.get(method, 0.33)
            
            if category not in category_votes:
                category_votes[category] = 0
            category_votes[category] += weight * confidence
        
        # æ‰¾åˆ°æœ€ä½³åˆ†ç±»
        if not category_votes or max(category_votes.values()) == 0:
            return {'category': 'Unknown', 'confidence': 0.0, 'probabilities': {}}
        
        best_category = max(category_votes, key=category_votes.get)
        
        # å½’ä¸€åŒ–æ¦‚ç‡
        total_votes = sum(category_votes.values())
        probabilities = {k: v/total_votes for k, v in category_votes.items()}
        
        return {
            'category': best_category,
            'confidence': probabilities[best_category],
            'probabilities': probabilities
        }
    
    def _generate_explanation(self, final_result: Dict, sub_results: Dict) -> str:
        """ç”Ÿæˆåˆ†ç±»è§£é‡Š"""
        category = final_result['category']
        confidence = final_result['confidence']
        
        if category == 'Unknown':
            return "æ— æ³•æ˜ç¡®åˆ†ç±»ï¼Œæ–‡æœ¬ä¿¡æ¯ä¸è¶³æˆ–ä¸åŒ¹é…ä»»ä½•å·²çŸ¥ç±»åˆ«"
        
        explanation = f"åˆ†ç±»ä¸º'{self.category_definitions[category]['name']}'ï¼Œç½®ä¿¡åº¦ï¼š{confidence:.2%}\n"
        
        # æ·»åŠ å„æ–¹æ³•çš„è´¡çŒ®
        for method, result in sub_results.items():
            method_name = {'rule_based': 'è§„åˆ™æ–¹æ³•', 'density_based': 'å¯†åº¦æ–¹æ³•', 'semantic_based': 'è¯­ä¹‰æ–¹æ³•'}[method]
            explanation += f"- {method_name}: {result['category']} (ç½®ä¿¡åº¦: {result['confidence']:.2%})\n"
        
        return explanation
    
    def batch_classify(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†ç±»"""
        results = []
        for i, text in enumerate(texts):
            print(f"æ­£åœ¨å¤„ç† {i+1}/{len(texts)}: {text[:50]}...")
            result = self.classify_text(text)
            results.append(result)
        return results
    
    def analyze_dataset(self, df: pd.DataFrame, text_column: str = 'Fn05601') -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªæ•°æ®é›†"""
        print("ğŸ” å¼€å§‹åˆ†ææ•°æ®é›†...")
        
        # æ‰¹é‡åˆ†ç±»
        texts = df[text_column].fillna('').tolist()
        results = self.batch_classify(texts)
        
        # ç»Ÿè®¡åˆ†æ
        predictions = [r['prediction'] for r in results]
        confidences = [r['confidence'] for r in results]
        
        # åˆ›å»ºç»“æœDataFrame
        result_df = df.copy()
        result_df['ml_prediction'] = predictions
        result_df['ml_confidence'] = confidences
        result_df['ml_prediction_cn'] = [self.category_definitions.get(p, {}).get('name', p) for p in predictions]
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        analysis = {
            'total_samples': len(texts),
            'category_distribution': Counter(predictions),
            'average_confidence': np.mean(confidences),
            'high_confidence_samples': sum(1 for c in confidences if c > 0.7),
            'low_confidence_samples': sum(1 for c in confidences if c < 0.3),
            'detailed_results': results,
            'result_dataframe': result_df
        }
        
        return analysis
    
    def create_visualizations(self, analysis: Dict[str, Any]):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. åˆ†ç±»åˆ†å¸ƒé¥¼å›¾
        category_dist = analysis['category_distribution']
        labels = [self.category_definitions.get(cat, {}).get('name', cat) for cat in category_dist.keys()]
        axes[0,0].pie(category_dist.values(), labels=labels, autopct='%1.1f%%')
        axes[0,0].set_title('è¡¥è´´åˆ†ç±»åˆ†å¸ƒ')
        
        # 2. ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾
        confidences = [r['confidence'] for r in analysis['detailed_results']]
        axes[0,1].hist(confidences, bins=20, alpha=0.7, color='skyblue')
        axes[0,1].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒ')
        axes[0,1].set_xlabel('ç½®ä¿¡åº¦')
        axes[0,1].set_ylabel('é¢‘æ¬¡')
        
        # 3. å„ç±»åˆ«å¹³å‡ç½®ä¿¡åº¦
        category_confidence = {}
        for result in analysis['detailed_results']:
            cat = result['prediction']
            if cat not in category_confidence:
                category_confidence[cat] = []
            category_confidence[cat].append(result['confidence'])
        
        avg_confidence = {cat: np.mean(conf_list) for cat, conf_list in category_confidence.items()}
        cat_names = [self.category_definitions.get(cat, {}).get('name', cat) for cat in avg_confidence.keys()]
        
        axes[1,0].bar(range(len(avg_confidence)), avg_confidence.values())
        axes[1,0].set_xticks(range(len(avg_confidence)))
        axes[1,0].set_xticklabels(cat_names, rotation=45)
        axes[1,0].set_title('å„ç±»åˆ«å¹³å‡ç½®ä¿¡åº¦')
        axes[1,0].set_ylabel('å¹³å‡ç½®ä¿¡åº¦')
        
        # 4. æ–¹æ³•è´¡çŒ®åº¦åˆ†æ
        method_accuracy = {'rule_based': 0, 'density_based': 0, 'semantic_based': 0}
        for result in analysis['detailed_results']:
            final_pred = result['prediction']
            for method, sub_result in result['sub_results'].items():
                if sub_result['category'] == final_pred:
                    method_accuracy[method] += 1
        
        method_names = ['è§„åˆ™æ–¹æ³•', 'å¯†åº¦æ–¹æ³•', 'è¯­ä¹‰æ–¹æ³•']
        method_values = [method_accuracy[m] for m in ['rule_based', 'density_based', 'semantic_based']]
        
        axes[1,1].bar(method_names, method_values)
        axes[1,1].set_title('å„æ–¹æ³•å¯¹æœ€ç»ˆå†³ç­–çš„è´¡çŒ®')
        axes[1,1].set_ylabel('è´¡çŒ®æ¬¡æ•°')
        
        plt.tight_layout()
        plt.savefig('../output/advanced_ml_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return fig

class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def process(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if pd.isna(text):
            return ""
        
        text = str(text).strip()
        
        # å»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡ã€æ•°å­—ã€å­—æ¯
        text = re.sub(r'[^\u4e00-\u9fff\w\s]', ' ', text)
        
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        return text

class FeatureExtractor:
    """ç‰¹å¾æå–å™¨"""
    
    def __init__(self, category_definitions):
        self.category_definitions = category_definitions
    
    def extract(self, text: str) -> Dict[str, Any]:
        """æå–æ–‡æœ¬ç‰¹å¾"""
        features = {}
        
        # åŸºç¡€ç‰¹å¾
        features['text_length'] = len(text)
        features['word_count'] = len(text.split())
        features['char_count'] = len(text)
        
        # å…³é”®è¯ç‰¹å¾
        for category, info in self.category_definitions.items():
            keyword_count = sum(1 for keyword in info['keywords'] if keyword in text)
            features[f'{category}_keyword_count'] = keyword_count
            features[f'{category}_keyword_density'] = keyword_count / max(len(text.split()), 1)
        
        # ç‰¹æ®Šç‰¹å¾
        features['has_numbers'] = int(bool(re.search(r'\d', text)))
        features['number_count'] = len(re.findall(r'\d+', text))
        features['has_year'] = int(bool(re.search(r'20\d{2}', text)))
        
        return features

class EnsembleClassifier:
    """é›†æˆåˆ†ç±»å™¨"""
    
    def __init__(self):
        pass
    
    def predict(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """é›†æˆé¢„æµ‹"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„é›†æˆé€»è¾‘
        return {'prediction': 'Unknown', 'confidence': 0.0}

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ”¿åºœè¡¥è´´ä¿¡æ¯æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = AdvancedSubsidyClassifier()
    
    # åŠ è½½æ•°æ®
    print("ğŸ“ åŠ è½½æ•°æ®...")
    df = pd.read_csv('../æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv')
    df.columns = ['Stkcd', 'Year', 'Fn05601', 'Fn05602', 'åˆè®¡', 'æ”¿åºœè¡¥è´´', 'Sum', 'test', 'Test']
    print(f"   æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # åˆ†ææ•°æ®é›†
    analysis = classifier.analyze_dataset(df)
    
    # æ‰“å°åˆ†ææŠ¥å‘Š
    print("\nğŸ“Š åˆ†ææŠ¥å‘Š:")
    print("=" * 40)
    print(f"æ€»æ ·æœ¬æ•°: {analysis['total_samples']}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {analysis['average_confidence']:.2%}")
    print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬ (>70%): {analysis['high_confidence_samples']}")
    print(f"ä½ç½®ä¿¡åº¦æ ·æœ¬ (<30%): {analysis['low_confidence_samples']}")
    
    print("\nğŸ“‹ åˆ†ç±»åˆ†å¸ƒ:")
    for category, count in analysis['category_distribution'].items():
        category_name = classifier.category_definitions.get(category, {}).get('name', category)
        percentage = count / analysis['total_samples'] * 100
        print(f"   {category_name}: {count} ({percentage:.1f}%)")
    
    # åˆ›å»ºå¯è§†åŒ–
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    classifier.create_visualizations(analysis)
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
    result_df = analysis['result_dataframe']
    result_df.to_csv('../output/æ”¿åºœè¡¥è´´æ•°æ®_æ™ºèƒ½åˆ†ç±»ç»“æœ.csv', index=False)
    
    # ä¿å­˜è¯¦ç»†åˆ†ææŠ¥å‘Š
    with open('../output/æ™ºèƒ½åˆ†ç±»åˆ†ææŠ¥å‘Š.json', 'w', encoding='utf-8') as f:
        # ç§»é™¤ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡
        serializable_analysis = {
            'total_samples': analysis['total_samples'],
            'category_distribution': dict(analysis['category_distribution']),
            'average_confidence': analysis['average_confidence'],
            'high_confidence_samples': analysis['high_confidence_samples'],
            'low_confidence_samples': analysis['low_confidence_samples']
        }
        json.dump(serializable_analysis, f, ensure_ascii=False, indent=2)
    
    # æµ‹è¯•é¢„æµ‹
    print("\nğŸ”® é¢„æµ‹ç¤ºä¾‹:")
    test_cases = [
        "é«˜æ–°æŠ€æœ¯ä¼ä¸šç ”å‘èµ„åŠ©",
        "å·¥ä¸šæŠ€æœ¯æ”¹é€ è¡¥è´´", 
        "å¤§å­¦ç”Ÿå°±ä¸šåˆ›ä¸šè¡¥åŠ©",
        "èŠ‚èƒ½ç¯ä¿é¡¹ç›®è¡¥è´´",
        "ä¸­å°ä¼ä¸šå‘å±•èµ„é‡‘",
        "ä¸“åˆ©ç”³è¯·èµ„åŠ©è´¹ç”¨"
    ]
    
    for text in test_cases:
        result = classifier.classify_text(text)
        category_name = classifier.category_definitions.get(result['prediction'], {}).get('name', result['prediction'])
        print(f"   '{text}' -> {category_name} (ç½®ä¿¡åº¦: {result['confidence']:.2%})")
    
    print("\nâœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:")
    print("   - output/æ”¿åºœè¡¥è´´æ•°æ®_æ™ºèƒ½åˆ†ç±»ç»“æœ.csv")
    print("   - output/æ™ºèƒ½åˆ†ç±»åˆ†ææŠ¥å‘Š.json")
    print("   - output/advanced_ml_analysis.png")

if __name__ == "__main__":
    main()