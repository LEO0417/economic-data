{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# æ”¿åºœè¡¥è´´æ•°æ®åˆ†æå®Œæ•´æµç¨‹\n",
        "\n",
        "æœ¬Jupyter NotebookåŒ…å«äº†æ”¿åºœè¡¥è´´æ•°æ®åˆ†æçš„å®Œæ•´æµç¨‹ï¼Œä»æ•°æ®é‡‡æ ·åˆ°æ·±åº¦å­¦ä¹ åˆ†ç±»çš„å…¨å¥—æ–¹æ³•ã€‚\n",
        "\n",
        "## é¡¹ç›®æ¦‚è¿°\n",
        "- **ç›®æ ‡**: å¯¹æ”¿åºœè¡¥è´´æ•°æ®è¿›è¡Œæ™ºèƒ½åˆ†ç±»å’Œåˆ†æ\n",
        "- **æ•°æ®**: æ”¿åºœè¡¥è´´æ•°æ®ï¼ˆ.dtaæ ¼å¼ï¼‰\n",
        "- **æ–¹æ³•**: è§„åˆ™åˆ†æ + æœºå™¨å­¦ä¹  + æ·±åº¦å­¦ä¹ \n",
        "- **è¾“å‡º**: åˆ†ç±»ç»“æœã€ç»Ÿè®¡æŠ¥å‘Šã€å¯è§†åŒ–å›¾è¡¨\n",
        "\n",
        "## åˆ†ææµç¨‹\n",
        "1. éšæœºé‡‡æ · - ä»å¤§æ•°æ®ä¸­æŠ½å–æ ·æœ¬\n",
        "2. æ•°æ®éªŒè¯ - æ£€æŸ¥æ•°æ®è´¨é‡\n",
        "3. æ ¼å¼è½¬æ¢ - è½¬ä¸ºCSVæ ¼å¼\n",
        "4. æ•°æ®é¢„è§ˆ - äº†è§£æ•°æ®ç»“æ„\n",
        "5. è§„åˆ™åˆ†æ - åŸºäºå…³é”®è¯åˆ†ç±»\n",
        "6. æœºå™¨å­¦ä¹ åˆ†ç±» - å¤šæ¨¡å‹é›†æˆ\n",
        "7. BERTæ™ºèƒ½åˆ†ç±» - æ·±åº¦å­¦ä¹ æ–¹æ³•\n",
        "\n",
        "---\n",
        "\n",
        "## ç¯å¢ƒè¦æ±‚\n",
        "```bash\n",
        "pip install pandas numpy matplotlib seaborn scikit-learn xgboost lightgbm jieba\n",
        "```\n",
        "\n",
        "å¼€å§‹åˆ†æå‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥å¿…è¦çš„åº“\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"ğŸ“š ç¯å¢ƒè®¾ç½®å®Œæˆï¼Œå¼€å§‹æ”¿åºœè¡¥è´´æ•°æ®åˆ†ææµç¨‹ï¼\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤1: éšæœºé‡‡æ ·\n",
        "\n",
        "**ç›®æ ‡**: ä»åŸå§‹å¤§æ•°æ®ä¸­æŠ½å–åƒåˆ†ä¹‹ä¸€çš„æ ·æœ¬ï¼Œä¾¿äºé«˜æ•ˆåˆ†æ\n",
        "\n",
        "**ä¸»è¦åŠŸèƒ½**:\n",
        "- è¯»å–.dtaæ ¼å¼çš„åŸå§‹æ•°æ®\n",
        "- éšæœºæŠ½å–0.1%çš„æ ·æœ¬æ•°æ®\n",
        "- å¤„ç†ä¸­æ–‡åˆ—åï¼Œç”Ÿæˆè‹±æ–‡æ˜ å°„\n",
        "- ä¿å­˜æ ·æœ¬æ•°æ®å’Œåˆ—åæ˜ å°„æ–‡ä»¶\n",
        "\n",
        "**è¾“å…¥**: `data/æ”¿åºœè¡¥è´´æ•°æ®.dta`  \n",
        "**è¾“å‡º**: `data/æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.dta`, `config/æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬_åˆ—åæ˜ å°„.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_sample_dta(input_file, output_file, sample_ratio=0.001):\n",
        "    \"\"\"\n",
        "    ä».dtaæ–‡ä»¶ä¸­éšæœºæå–æ ·æœ¬\n",
        "    \n",
        "    å‚æ•°:\n",
        "    input_file (str): è¾“å…¥çš„.dtaæ–‡ä»¶è·¯å¾„\n",
        "    output_file (str): è¾“å‡ºçš„.dtaæ–‡ä»¶è·¯å¾„\n",
        "    sample_ratio (float): é‡‡æ ·æ¯”ä¾‹ï¼Œé»˜è®¤ä¸º0.001ï¼ˆåƒåˆ†ä¹‹ä¸€ï¼‰\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"ğŸ” æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶: {input_file}\")\n",
        "    \n",
        "    try:\n",
        "        # è¯»å–.dtaæ–‡ä»¶\n",
        "        df = pd.read_stata(input_file)\n",
        "        \n",
        "        print(f\"ğŸ“Š åŸå§‹æ•°æ®é›†å¤§å°: {len(df)} è¡Œ, {len(df.columns)} åˆ—\")\n",
        "        \n",
        "        # è®¡ç®—æ ·æœ¬å¤§å°\n",
        "        sample_size = int(len(df) * sample_ratio)\n",
        "        print(f\"ğŸ¯ å°†è¦æå–çš„æ ·æœ¬å¤§å°: {sample_size} è¡Œ\")\n",
        "        \n",
        "        # éšæœºè®¾ç½®ç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # éšæœºé‡‡æ ·\n",
        "        sampled_df = df.sample(n=sample_size, random_state=42)\n",
        "        \n",
        "        print(f\"âœ… éšæœºé‡‡æ ·å®Œæˆï¼Œæ ·æœ¬å¤§å°: {len(sampled_df)} è¡Œ\")\n",
        "        \n",
        "        # å¤„ç†ä¸­æ–‡åˆ—åï¼Œé‡å‘½åä¸ºè‹±æ–‡ä»¥å…¼å®¹Stataæ ¼å¼\n",
        "        column_mapping = {}\n",
        "        for i, col in enumerate(sampled_df.columns):\n",
        "            # åˆ›å»ºè‹±æ–‡åˆ—åæ˜ å°„\n",
        "            if any('\\u4e00' <= char <= '\\u9fff' for char in str(col)):\n",
        "                new_col_name = f\"var_{i+1}\"\n",
        "                column_mapping[col] = new_col_name\n",
        "                print(f\"ğŸ”„ åˆ—åæ˜ å°„: '{col}' -> '{new_col_name}'\")\n",
        "        \n",
        "        # é‡å‘½ååˆ—\n",
        "        if column_mapping:\n",
        "            sampled_df = sampled_df.rename(columns=column_mapping)\n",
        "            print(f\"ğŸ“ å·²é‡å‘½å {len(column_mapping)} ä¸ªåŒ…å«ä¸­æ–‡çš„åˆ—å\")\n",
        "            \n",
        "            # ä¿å­˜åˆ—åæ˜ å°„åˆ°æ–‡ä»¶\n",
        "            mapping_file = \"config/\" + os.path.basename(output_file).replace('.dta', '_åˆ—åæ˜ å°„.txt')\n",
        "            os.makedirs(\"config\", exist_ok=True)\n",
        "            with open(mapping_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(\"åŸå§‹åˆ—å -> æ–°åˆ—å\\n\")\n",
        "                f.write(\"=\" * 30 + \"\\n\")\n",
        "                for old_name, new_name in column_mapping.items():\n",
        "                    f.write(f\"{old_name} -> {new_name}\\n\")\n",
        "            print(f\"ğŸ’¾ åˆ—åæ˜ å°„å·²ä¿å­˜åˆ°: {mapping_file}\")\n",
        "        \n",
        "        # ä¿å­˜ä¸ºæ–°çš„.dtaæ–‡ä»¶\n",
        "        try:\n",
        "            sampled_df.to_stata(output_file, write_index=False, version=118)\n",
        "            print(f\"ğŸ’¾ æ ·æœ¬æ•°æ®å·²ä¿å­˜åˆ°: {output_file}\")\n",
        "        except UnicodeEncodeError:\n",
        "            # å¦‚æœæœ‰ç¼–ç é—®é¢˜ï¼Œä¿å­˜ä¸ºCSVæ ¼å¼\n",
        "            csv_file = output_file.replace('.dta', '.csv')\n",
        "            sampled_df.to_csv(csv_file, index=False, encoding='utf-8-sig')\n",
        "            print(f\"ğŸ’¾ å·²ä¿å­˜ä¸ºCSVæ ¼å¼: {csv_file}\")\n",
        "        \n",
        "        # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
        "        print(f\"\\nğŸ“ˆ æ ·æœ¬æ•°æ®åŸºæœ¬ä¿¡æ¯:\")\n",
        "        print(f\"   - æ•°æ®å½¢çŠ¶: {sampled_df.shape}\")\n",
        "        print(f\"   - åˆ—å: {list(sampled_df.columns)}\")\n",
        "        \n",
        "        return sampled_df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# æ‰§è¡Œéšæœºé‡‡æ ·\n",
        "print(\"ğŸš€ æ­¥éª¤1: å¼€å§‹éšæœºé‡‡æ ·\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "input_file = \"data/æ”¿åºœè¡¥è´´æ•°æ®.dta\"\n",
        "output_file = \"data/æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.dta\"\n",
        "\n",
        "# æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "if os.path.exists(input_file):\n",
        "    sample_data = random_sample_dta(input_file, output_file, sample_ratio=0.001)\n",
        "    if sample_data is not None:\n",
        "        print(f\"\\nâœ… æ­¥éª¤1å®Œæˆï¼æ ·æœ¬æ•°æ®å·²ç”Ÿæˆ\")\n",
        "        print(f\"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
        "        \n",
        "        # æ˜¾ç¤ºæ ·æœ¬æ•°æ®çš„å‰å‡ è¡Œ\n",
        "        print(f\"\\nğŸ‘€ æ ·æœ¬æ•°æ®é¢„è§ˆï¼ˆå‰5è¡Œï¼‰:\")\n",
        "        display(sample_data.head())\n",
        "    else:\n",
        "        print(\"âŒ é‡‡æ ·å¤±è´¥ï¼\")\n",
        "else:\n",
        "    print(f\"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}\")\n",
        "    print(\"âš ï¸  è¯·ç¡®ä¿åŸå§‹æ•°æ®æ–‡ä»¶ä½äº data/ ç›®å½•ä¸­\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤2: æ ·æœ¬æ•°æ®éªŒè¯\n",
        "\n",
        "**ç›®æ ‡**: éªŒè¯æ ·æœ¬æ•°æ®çš„è´¨é‡å’Œç»“æ„ï¼Œç¡®ä¿æ•°æ®å¯ç”¨æ€§\n",
        "\n",
        "**ä¸»è¦åŠŸèƒ½**:\n",
        "- æ£€æŸ¥æ ·æœ¬æ•°æ®çš„åŸºæœ¬ä¿¡æ¯ï¼ˆè¡Œåˆ—æ•°ã€æ•°æ®ç±»å‹ï¼‰\n",
        "- ç»Ÿè®¡æ•°å€¼åˆ—çš„æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯\n",
        "- è®¡ç®—æ–‡ä»¶å‹ç¼©æ¯”ä¾‹\n",
        "- æ£€æŸ¥æ•°æ®å®Œæ•´æ€§\n",
        "\n",
        "**è¾“å…¥**: `data/æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.dta`  \n",
        "**è¾“å‡º**: ç»ˆç«¯æŠ¥å‘Šï¼ˆæ— æ–‡ä»¶è¾“å‡ºï¼‰\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_sample_data():\n",
        "    \"\"\"éªŒè¯æ ·æœ¬æ•°æ®\"\"\"\n",
        "    \n",
        "    sample_file = \"data/æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.dta\"\n",
        "    original_file = \"data/æ”¿åºœè¡¥è´´æ•°æ®.dta\"\n",
        "    \n",
        "    if not os.path.exists(sample_file):\n",
        "        print(f\"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ ·æœ¬æ–‡ä»¶ {sample_file}\")\n",
        "        return\n",
        "    \n",
        "    print(\"ğŸ” æ­¥éª¤2: æ ·æœ¬æ•°æ®éªŒè¯\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # è¯»å–æ ·æœ¬æ•°æ®\n",
        "    print(\"ğŸ“– æ­£åœ¨è¯»å–æ ·æœ¬æ•°æ®...\")\n",
        "    sample_df = pd.read_stata(sample_file)\n",
        "    \n",
        "    print(f\"ğŸ“Š æ ·æœ¬æ•°æ®å¤§å°: {sample_df.shape}\")\n",
        "    print(f\"ğŸ“‹ åˆ—å: {list(sample_df.columns)}\")\n",
        "    \n",
        "    # æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
        "    print(f\"\\nğŸ“ˆ æ•°å€¼åˆ—çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:\")\n",
        "    numeric_columns = sample_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    if len(numeric_columns) > 0:\n",
        "        display(sample_df[numeric_columns].describe())\n",
        "    \n",
        "    # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®\n",
        "    print(f\"\\nğŸ‘€ æ ·æœ¬æ•°æ®å‰5è¡Œ:\")\n",
        "    display(sample_df.head())\n",
        "    \n",
        "    # æ£€æŸ¥æ•°æ®ç±»å‹\n",
        "    print(f\"\\nğŸ” å„åˆ—æ•°æ®ç±»å‹:\")\n",
        "    print(sample_df.dtypes)\n",
        "    \n",
        "    # æ£€æŸ¥ç¼ºå¤±å€¼\n",
        "    print(f\"\\nâ“ ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n",
        "    missing_info = sample_df.isnull().sum()\n",
        "    missing_percent = (missing_info / len(sample_df)) * 100\n",
        "    missing_df = pd.DataFrame({\n",
        "        'ç¼ºå¤±æ•°é‡': missing_info,\n",
        "        'ç¼ºå¤±æ¯”ä¾‹(%)': missing_percent.round(2)\n",
        "    })\n",
        "    print(missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0])\n",
        "    \n",
        "    # æ–‡ä»¶å¤§å°æ¯”è¾ƒ\n",
        "    if os.path.exists(original_file):\n",
        "        sample_size = os.path.getsize(sample_file) / (1024*1024)\n",
        "        original_size = os.path.getsize(original_file) / (1024*1024)\n",
        "        \n",
        "        print(f\"\\nğŸ“ æ–‡ä»¶å¤§å°æ¯”è¾ƒ:\")\n",
        "        print(f\"   åŸå§‹æ–‡ä»¶: {original_size:.2f} MB\")\n",
        "        print(f\"   æ ·æœ¬æ–‡ä»¶: {sample_size:.2f} MB\")\n",
        "        print(f\"   å‹ç¼©æ¯”ä¾‹: {(sample_size/original_size)*100:.3f}%\")\n",
        "    \n",
        "    print(f\"\\nâœ… æ­¥éª¤2å®Œæˆï¼æ ·æœ¬æ•°æ®éªŒè¯é€šè¿‡\")\n",
        "    return sample_df\n",
        "\n",
        "# æ‰§è¡Œæ ·æœ¬æ•°æ®éªŒè¯\n",
        "verified_data = verify_sample_data()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤3: æ ¼å¼è½¬æ¢\n",
        "\n",
        "**ç›®æ ‡**: å°†Stataæ ¼å¼ï¼ˆ.dtaï¼‰è½¬æ¢ä¸ºCSVæ ¼å¼ï¼Œä¾¿äºPythonåˆ†æ\n",
        "\n",
        "**ä¸»è¦åŠŸèƒ½**:\n",
        "- è¯»å–.dtaæ ¼å¼çš„æ ·æœ¬æ•°æ®\n",
        "- è½¬æ¢ä¸ºCSVæ ¼å¼å¹¶ä¿å­˜\n",
        "- ä¿æŒæ•°æ®å®Œæ•´æ€§å’Œç¼–ç æ­£ç¡®æ€§\n",
        "\n",
        "**è¾“å…¥**: `data/æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.dta`  \n",
        "**è¾“å‡º**: `output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_dta_to_csv(input_file, output_file):\n",
        "    \"\"\"\n",
        "    å°†Stataæ–‡ä»¶è½¬æ¢ä¸ºCSVæ ¼å¼\n",
        "    \n",
        "    å‚æ•°:\n",
        "    input_file (str): è¾“å…¥çš„.dtaæ–‡ä»¶è·¯å¾„\n",
        "    output_file (str): è¾“å‡ºçš„.csvæ–‡ä»¶è·¯å¾„\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {input_file}\")\n",
        "        \n",
        "        # è¯»å–dtaæ–‡ä»¶\n",
        "        df = pd.read_stata(input_file)\n",
        "        \n",
        "        print(f\"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
        "        print(f\"ğŸ“‹ åˆ—å: {list(df.columns)}\")\n",
        "        \n",
        "        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
        "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "        \n",
        "        # ä¿å­˜ä¸ºCSV\n",
        "        df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "        \n",
        "        print(f\"ğŸ’¾ è½¬æ¢å®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file}\")\n",
        "        print(f\"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# æ‰§è¡Œæ ¼å¼è½¬æ¢\n",
        "print(\"ğŸ”„ æ­¥éª¤3: æ ¼å¼è½¬æ¢\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "input_file = \"data/æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.dta\"\n",
        "output_file = \"output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv\"\n",
        "\n",
        "# æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "if os.path.exists(input_file):\n",
        "    success = convert_dta_to_csv(input_file, output_file)\n",
        "    \n",
        "    if success:\n",
        "        print(f\"\\nâœ… æ­¥éª¤3å®Œæˆï¼CSVæ–‡ä»¶å·²ç”Ÿæˆ\")\n",
        "        \n",
        "        # è¯»å–å¹¶é¢„è§ˆè½¬æ¢åçš„CSVæ•°æ®\n",
        "        csv_df = pd.read_csv(output_file)\n",
        "        print(f\"\\nğŸ‘€ è½¬æ¢åçš„CSVæ•°æ®é¢„è§ˆï¼ˆå‰3è¡Œï¼‰:\")\n",
        "        display(csv_df.head(3))\n",
        "    else:\n",
        "        print(f\"\\nâŒ æ­¥éª¤3å¤±è´¥ï¼\")\n",
        "else:\n",
        "    print(f\"âŒ æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {input_file}\")\n",
        "    print(\"âš ï¸  è¯·å…ˆè¿è¡Œæ­¥éª¤1ç”Ÿæˆæ ·æœ¬æ•°æ®\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤4: æ•°æ®é¢„è§ˆ\n",
        "\n",
        "**ç›®æ ‡**: æ·±å…¥äº†è§£CSVæ•°æ®çš„ç»“æ„ã€ç‰¹å¾å’Œè´¨é‡\n",
        "\n",
        "**ä¸»è¦åŠŸèƒ½**:\n",
        "- æ˜¾ç¤ºæ•°æ®ç»´åº¦å’ŒåŸºæœ¬ä¿¡æ¯\n",
        "- æ£€æŸ¥ç¼ºå¤±å€¼åˆ†å¸ƒ\n",
        "- æŸ¥çœ‹æ•°æ®ç±»å‹å’Œå†…å­˜ä½¿ç”¨\n",
        "- é¢„è§ˆæ•°æ®æ ·ä¾‹\n",
        "\n",
        "**è¾“å…¥**: `output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv`  \n",
        "**è¾“å‡º**: è¯¦ç»†çš„æ•°æ®é¢„è§ˆæŠ¥å‘Š\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preview_csv(filename='output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv'):\n",
        "    \"\"\"é¢„è§ˆCSVæ–‡ä»¶\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"âŒ æ–‡ä»¶ {filename} ä¸å­˜åœ¨\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        # è¯»å–CSVæ–‡ä»¶\n",
        "        df = pd.read_csv(filename)\n",
        "        \n",
        "        print(\"ğŸ‘€ æ­¥éª¤4: æ•°æ®é¢„è§ˆ\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"ğŸ“Š æ•°æ®æ–‡ä»¶: {filename}\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        # åŸºæœ¬ä¿¡æ¯\n",
        "        print(f\"ğŸ“ æ•°æ®ç»´åº¦: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—\")\n",
        "        print(f\"ğŸ’¾ å†…å­˜ä½¿ç”¨: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
        "        print()\n",
        "        \n",
        "        # åˆ—åä¿¡æ¯\n",
        "        print(\"ğŸ“‹ åˆ—ååˆ—è¡¨:\")\n",
        "        for i, col in enumerate(df.columns, 1):\n",
        "            print(f\"  {i:2d}. {col}\")\n",
        "        print()\n",
        "        \n",
        "        # æ•°æ®ç±»å‹\n",
        "        print(\"ğŸ” æ•°æ®ç±»å‹:\")\n",
        "        print(df.dtypes)\n",
        "        print()\n",
        "        \n",
        "        # ç¼ºå¤±å€¼ç»Ÿè®¡\n",
        "        print(\"â“ ç¼ºå¤±å€¼ç»Ÿè®¡:\")\n",
        "        missing = df.isnull().sum()\n",
        "        missing_percent = (missing / len(df)) * 100\n",
        "        missing_info = pd.DataFrame({\n",
        "            'ç¼ºå¤±æ•°é‡': missing,\n",
        "            'ç¼ºå¤±æ¯”ä¾‹(%)': missing_percent.round(2)\n",
        "        })\n",
        "        missing_summary = missing_info[missing_info['ç¼ºå¤±æ•°é‡'] > 0]\n",
        "        if len(missing_summary) > 0:\n",
        "            display(missing_summary)\n",
        "        else:\n",
        "            print(\"   æ— ç¼ºå¤±å€¼ âœ…\")\n",
        "        print()\n",
        "        \n",
        "        # å‰5è¡Œæ•°æ®\n",
        "        print(\"ğŸ‘€ å‰5è¡Œæ•°æ®é¢„è§ˆ:\")\n",
        "        display(df.head())\n",
        "        print()\n",
        "        \n",
        "        # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…æ•°å€¼åˆ—ï¼‰\n",
        "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
        "        if len(numeric_cols) > 0:\n",
        "            print(\"ğŸ“ˆ æ•°å€¼åˆ—åŸºæœ¬ç»Ÿè®¡:\")\n",
        "            display(df[numeric_cols].describe())\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "        print(\"âœ… æ­¥éª¤4å®Œæˆï¼æ•°æ®é¢„è§ˆç»“æŸ\")\n",
        "        \n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}\")\n",
        "        return None\n",
        "\n",
        "# æ‰§è¡Œæ•°æ®é¢„è§ˆ\n",
        "preview_data = preview_csv()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤5: åŸºäºè§„åˆ™çš„è¡¥è´´åˆ†æ\n",
        "\n",
        "**ç›®æ ‡**: ä½¿ç”¨é¢„å®šä¹‰çš„å…³é”®è¯è§„åˆ™å¯¹æ”¿åºœè¡¥è´´è¿›è¡Œåˆ†ç±»å’Œç»Ÿè®¡åˆ†æ\n",
        "\n",
        "**ä¸»è¦åŠŸèƒ½**:\n",
        "- å®šä¹‰è¡¥è´´åˆ†ç±»è§„åˆ™ï¼ˆç ”å‘åˆ›æ–°ã€å·¥ä¸šè®¾å¤‡ã€å°±ä¸šã€ç¯ä¿ç­‰ï¼‰\n",
        "- åŸºäºå…³é”®è¯å¯¹è¡¥è´´æè¿°è¿›è¡Œè‡ªåŠ¨åˆ†ç±»\n",
        "- ç»Ÿè®¡å„ç±»åˆ«çš„æ•°é‡ã€é‡‘é¢ã€ä¼ä¸šæ•°å’Œå æ¯”\n",
        "- åˆ†æå¹´åº¦è¡¥è´´è¶‹åŠ¿å’Œå…³é”®è¯é¢‘æ¬¡\n",
        "- ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨\n",
        "\n",
        "**è¾“å…¥**: `output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv`  \n",
        "**è¾“å‡º**: \n",
        "- `output/5_æ”¿åºœè¡¥è´´æ•°æ®_åˆ†æç»“æœ.csv`\n",
        "- `output/5_è¡¥è´´ç±»åˆ«ç»Ÿè®¡.csv`\n",
        "- `output/5_å¹´åº¦è¡¥è´´ç»Ÿè®¡.csv`\n",
        "- `output/5_subsidy_analysis.png`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯¼å…¥é¢å¤–çš„åˆ†æåº“\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def load_data(filename='output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv'):\n",
        "    \"\"\"åŠ è½½æ•°æ®\"\"\"\n",
        "    df = pd.read_csv(filename)\n",
        "    return df\n",
        "\n",
        "def classify_subsidies(df):\n",
        "    \"\"\"\n",
        "    æ ¹æ®å…³é”®è¯è§„åˆ™å¯¹è¡¥è´´è¿›è¡Œåˆ†ç±»\n",
        "    \"\"\"\n",
        "    # å®šä¹‰å…³é”®è¯å­—å…¸\n",
        "    keywords = {\n",
        "        'R&D_Innovation': [\n",
        "            'åˆ›æ–°', 'ç ”å‘', 'ä¸“åˆ©', 'ç§‘æŠ€', 'æŠ€æœ¯', 'çŸ¥è¯†äº§æƒ', 'ç ”ç©¶', \n",
        "            'å¼€å‘', 'ç§‘å­¦', 'å‘æ˜', 'é«˜æ–°', 'æ™ºèƒ½', 'æ•°å­—åŒ–', 'ä¿¡æ¯åŒ–'\n",
        "        ],\n",
        "        'Industrial_Equipment': [\n",
        "            'å·¥ä¸š', 'è®¾å¤‡', 'æŠ€æ”¹', 'æ”¹é€ ', 'å‡çº§', 'è½¬å‹', 'åˆ¶é€ ', \n",
        "            'ç”Ÿäº§çº¿', 'æœºæ¢°', 'è£…å¤‡', 'äº§ä¸šåŒ–'\n",
        "        ],\n",
        "        'Employment': [\n",
        "            'å°±ä¸š', 'æ‹›è˜', 'å®ä¹ ', 'åŸ¹è®­', 'ç¨³å²—', 'ç”¨å·¥', 'åŠ³åŠ¨', \n",
        "            'èŒä¸š', 'æ¯•ä¸šç”Ÿ', 'æ‰©å²—', 'äººæ‰'\n",
        "        ],\n",
        "        'Environment': [\n",
        "            'èŠ‚èƒ½', 'ç¯ä¿', 'æ¸…æ´', 'å‡æ’', 'æ±¡æŸ“', 'æ²»ç†', 'ç»¿è‰²', \n",
        "            'å¾ªç¯', 'ç”Ÿæ€', 'åºŸæ–™', 'æ’æ”¾'\n",
        "        ],\n",
        "        'General_Business': [\n",
        "            'ç»è¥', 'å‡ºå£', 'å“ç‰Œ', 'ç¨æ”¶', 'å‘å±•', 'å¸‚åœº', 'è´¸æ˜“', \n",
        "            'è¥ä¸š', 'å•†åŠ¡', 'è´¢æ”¿', 'å¥–åŠ±', 'æ‰¶æŒ'\n",
        "        ],\n",
        "        'Other': [],\n",
        "        'Unknown': ['å…¶ä»–', 'è¡¥åŠ©', 'è¡¥è´´', 'æ”¿åºœ']\n",
        "    }\n",
        "    \n",
        "    def classify_single_subsidy(description):\n",
        "        \"\"\"å¯¹å•ä¸ªè¡¥è´´æè¿°è¿›è¡Œåˆ†ç±»\"\"\"\n",
        "        if pd.isna(description):\n",
        "            return 'Unknown'\n",
        "        \n",
        "        description = str(description).lower()\n",
        "        \n",
        "        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒ¹é…åˆ†æ•°\n",
        "        scores = {}\n",
        "        for category, words in keywords.items():\n",
        "            if category == 'Other':\n",
        "                continue\n",
        "            score = sum(1 for word in words if word in description)\n",
        "            scores[category] = score\n",
        "        \n",
        "        # æ‰¾åˆ°æœ€é«˜åˆ†æ•°çš„ç±»åˆ«\n",
        "        if max(scores.values()) == 0:\n",
        "            return 'Unknown'\n",
        "        \n",
        "        return max(scores, key=scores.get)\n",
        "    \n",
        "    # å¯¹æ¯ä¸ªè¡¥è´´è¿›è¡Œåˆ†ç±»\n",
        "    df['subsidy_category'] = df['Fn05601'].apply(classify_single_subsidy)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def analyze_subsidy_distribution(df):\n",
        "    \"\"\"åˆ†æè¡¥è´´åˆ†å¸ƒ\"\"\"\n",
        "    print(\"ğŸ—ï¸ æ­¥éª¤5: åŸºäºè§„åˆ™çš„è¡¥è´´åˆ†æ\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"ğŸ“Š æ”¿åºœè¡¥è´´æ•°æ®åˆ†ææŠ¥å‘Š\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # åŸºæœ¬ç»Ÿè®¡\n",
        "    print(f\"ğŸ“ˆ æ•°æ®æ¦‚è§ˆ:\")\n",
        "    print(f\"   æ€»è®°å½•æ•°: {len(df):,}\")\n",
        "    print(f\"   æ—¶é—´è·¨åº¦: {df['Year'].min():.0f} - {df['Year'].max():.0f}\")\n",
        "    print(f\"   æ¶‰åŠä¼ä¸šæ•°: {df['Stkcd'].nunique():,}\")\n",
        "    print(f\"   è¡¥è´´æ€»é‡‘é¢: {df['Fn05602'].sum():,.0f} å…ƒ\")\n",
        "    print()\n",
        "    \n",
        "    # æŒ‰ç±»åˆ«ç»Ÿè®¡\n",
        "    category_stats = df.groupby('subsidy_category').agg({\n",
        "        'Fn05602': ['count', 'sum', 'mean'],\n",
        "        'Stkcd': 'nunique'\n",
        "    }).round(2)\n",
        "    \n",
        "    category_stats.columns = ['è¡¥è´´æ•°é‡', 'è¡¥è´´æ€»é¢', 'å¹³å‡è¡¥è´´é¢', 'æ¶‰åŠä¼ä¸šæ•°']\n",
        "    category_stats['å æ¯”(%)'] = (category_stats['è¡¥è´´æ•°é‡'] / len(df) * 100).round(2)\n",
        "    \n",
        "    print(\"ğŸ“‹ æŒ‰è¡¥è´´ç±»åˆ«ç»Ÿè®¡:\")\n",
        "    display(category_stats.sort_values('è¡¥è´´æ€»é¢', ascending=False))\n",
        "    print()\n",
        "    \n",
        "    # æŒ‰å¹´ä»½ç»Ÿè®¡\n",
        "    yearly_stats = df.groupby('Year').agg({\n",
        "        'Fn05602': ['count', 'sum'],\n",
        "        'Stkcd': 'nunique'\n",
        "    }).round(2)\n",
        "    yearly_stats.columns = ['è¡¥è´´æ•°é‡', 'è¡¥è´´æ€»é¢', 'æ¶‰åŠä¼ä¸šæ•°']\n",
        "    \n",
        "    print(\"ğŸ“… æŒ‰å¹´ä»½ç»Ÿè®¡ (å‰10å¹´):\")\n",
        "    display(yearly_stats.sort_values('è¡¥è´´æ€»é¢', ascending=False).head(10))\n",
        "    print()\n",
        "    \n",
        "    return category_stats, yearly_stats\n",
        "\n",
        "def analyze_keywords(df):\n",
        "    \"\"\"åˆ†æè¡¥è´´æè¿°ä¸­çš„å…³é”®è¯\"\"\"\n",
        "    print(\"ğŸ” è¡¥è´´æè¿°å…³é”®è¯åˆ†æ:\")\n",
        "    \n",
        "    # æå–æ‰€æœ‰è¡¥è´´æè¿°\n",
        "    all_descriptions = ' '.join(df['Fn05601'].dropna().astype(str))\n",
        "    \n",
        "    # å¸¸è§å…³é”®è¯\n",
        "    common_words = [\n",
        "        'è¡¥è´´', 'è¡¥åŠ©', 'èµ„é‡‘', 'å¥–åŠ±', 'ä¸“é¡¹', 'é¡¹ç›®', 'æŠ€æœ¯', 'å‘å±•',\n",
        "        'ä¼ä¸š', 'äº§ä¸š', 'åˆ›æ–°', 'ç ”å‘', 'ç§‘æŠ€', 'å·¥ä¸š', 'è´¢æ”¿', 'æ”¿åºœ'\n",
        "    ]\n",
        "    \n",
        "    word_counts = {}\n",
        "    for word in common_words:\n",
        "        count = all_descriptions.count(word)\n",
        "        word_counts[word] = count\n",
        "    \n",
        "    # æŒ‰å‡ºç°é¢‘æ¬¡æ’åº\n",
        "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    print(\"   å…³é”®è¯å‡ºç°é¢‘æ¬¡:\")\n",
        "    for word, count in sorted_words[:15]:\n",
        "        print(f\"   {word}: {count}\")\n",
        "    print()\n",
        "\n",
        "def create_visualizations(df, category_stats, yearly_stats):\n",
        "    \"\"\"åˆ›å»ºå¯è§†åŒ–å›¾è¡¨\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # 1. è¡¥è´´ç±»åˆ«åˆ†å¸ƒé¥¼å›¾\n",
        "    category_counts = df['subsidy_category'].value_counts()\n",
        "    axes[0,0].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
        "    axes[0,0].set_title('è¡¥è´´ç±»åˆ«åˆ†å¸ƒ')\n",
        "    \n",
        "    # 2. å¹´åº¦è¡¥è´´è¶‹åŠ¿\n",
        "    yearly_amount = df.groupby('Year')['Fn05602'].sum() / 1e8  # è½¬æ¢ä¸ºäº¿å…ƒ\n",
        "    axes[0,1].plot(yearly_amount.index, yearly_amount.values, marker='o')\n",
        "    axes[0,1].set_title('å¹´åº¦è¡¥è´´æ€»é¢è¶‹åŠ¿')\n",
        "    axes[0,1].set_xlabel('å¹´ä»½')\n",
        "    axes[0,1].set_ylabel('è¡¥è´´æ€»é¢(äº¿å…ƒ)')\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # 3. è¡¥è´´é‡‘é¢åˆ†å¸ƒç®±çº¿å›¾\n",
        "    df_plot = df[df['Fn05602'] > 0]  # æ’é™¤0å€¼\n",
        "    category_list = category_counts.index.tolist()\n",
        "    data_for_box = []\n",
        "    labels_for_box = []\n",
        "    \n",
        "    for cat in category_list:\n",
        "        cat_data = df_plot[df_plot['subsidy_category']==cat]['Fn05602']\n",
        "        if len(cat_data) > 0:\n",
        "            data_for_box.append(np.log10(cat_data))\n",
        "            labels_for_box.append(cat)\n",
        "    \n",
        "    if data_for_box:\n",
        "        axes[1,0].boxplot(data_for_box)\n",
        "        axes[1,0].set_xticklabels(labels_for_box, rotation=45)\n",
        "        axes[1,0].set_title('å„ç±»åˆ«è¡¥è´´é‡‘é¢åˆ†å¸ƒ(log10)')\n",
        "        axes[1,0].set_ylabel('è¡¥è´´é‡‘é¢(log10)')\n",
        "    \n",
        "    # 4. test vs Test äº¤å‰è¡¨\n",
        "    cross_tab = pd.crosstab(df['test'], df['Test'])\n",
        "    sns.heatmap(cross_tab, annot=True, fmt='d', ax=axes[1,1])\n",
        "    axes[1,1].set_title('test vs Test äº¤å‰åˆ†å¸ƒ')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('output/5_subsidy_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# æ‰§è¡Œè§„åˆ™åˆ†æ\n",
        "print(\"ğŸ—ï¸ å¼€å§‹æ‰§è¡Œæ­¥éª¤5: åŸºäºè§„åˆ™çš„è¡¥è´´åˆ†æ\")\n",
        "\n",
        "# åŠ è½½æ•°æ®\n",
        "df = load_data()\n",
        "\n",
        "# åˆ†ç±»è¡¥è´´\n",
        "df = classify_subsidies(df)\n",
        "\n",
        "# åˆ†æè¡¥è´´åˆ†å¸ƒ\n",
        "category_stats, yearly_stats = analyze_subsidy_distribution(df)\n",
        "\n",
        "# å…³é”®è¯åˆ†æ\n",
        "analyze_keywords(df)\n",
        "\n",
        "# åˆ›å»ºå¯è§†åŒ–\n",
        "create_visualizations(df, category_stats, yearly_stats)\n",
        "\n",
        "# ä¿å­˜åˆ†æç»“æœ\n",
        "os.makedirs('output', exist_ok=True)\n",
        "df.to_csv('output/5_æ”¿åºœè¡¥è´´æ•°æ®_åˆ†æç»“æœ.csv', index=False)\n",
        "category_stats.to_csv('output/5_è¡¥è´´ç±»åˆ«ç»Ÿè®¡.csv')\n",
        "yearly_stats.to_csv('output/5_å¹´åº¦è¡¥è´´ç»Ÿè®¡.csv')\n",
        "\n",
        "print(\"âœ… æ­¥éª¤5å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°ä»¥ä¸‹æ–‡ä»¶:\")\n",
        "print(\"   - output/5_æ”¿åºœè¡¥è´´æ•°æ®_åˆ†æç»“æœ.csv\")\n",
        "print(\"   - output/5_è¡¥è´´ç±»åˆ«ç»Ÿè®¡.csv\") \n",
        "print(\"   - output/5_å¹´åº¦è¡¥è´´ç»Ÿè®¡.csv\")\n",
        "print(\"   - output/5_subsidy_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤6: æœºå™¨å­¦ä¹ æ–‡æœ¬åˆ†ç±»\n",
        "\n",
        "**ç›®æ ‡**: ä½¿ç”¨å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•å¯¹è¡¥è´´æ–‡æœ¬è¿›è¡Œæ™ºèƒ½åˆ†ç±»\n",
        "\n",
        "**ä¸»è¦åŠŸèƒ½**:\n",
        "- æ–‡æœ¬é¢„å¤„ç†ï¼ˆä¸­æ–‡åˆ†è¯ã€ç‰¹å¾æå–ï¼‰\n",
        "- å¤šç§MLæ¨¡å‹è®­ç»ƒï¼ˆéšæœºæ£®æ—ã€XGBoostã€LightGBMã€SVMç­‰ï¼‰\n",
        "- æ¨¡å‹è¯„ä¼°å’Œæ¯”è¾ƒ\n",
        "- ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
        "- é›†æˆæ¨¡å‹é¢„æµ‹\n",
        "\n",
        "**è¾“å…¥**: `output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv`  \n",
        "**è¾“å‡º**: \n",
        "- `output/6_æ”¿åºœè¡¥è´´æ•°æ®_MLåˆ†ç±»ç»“æœ.csv`\n",
        "- `output/6_ml_classification_results.png`\n",
        "\n",
        "**æ³¨æ„**: éœ€è¦å®‰è£…jiebaã€scikit-learnã€xgboostã€lightgbmç­‰åº“\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®‰è£…å’Œå¯¼å…¥MLç›¸å…³åº“\n",
        "try:\n",
        "    import jieba\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    import xgboost as xgb\n",
        "    import lightgbm as lgb\n",
        "    \n",
        "    print(\"âœ… æ‰€æœ‰MLåº“å¯¼å…¥æˆåŠŸ\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ç¼ºå°‘å¿…è¦çš„åº“: {e}\")\n",
        "    print(\"è¯·è¿è¡Œ: pip install jieba scikit-learn xgboost lightgbm\")\n",
        "\n",
        "class TextClassifier:\n",
        "    def __init__(self):\n",
        "        self.tfidf = TfidfVectorizer(max_features=5000, stop_words=None, ngram_range=(1, 2))\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.models = {}\n",
        "        \n",
        "    def preprocess_text(self, texts):\n",
        "        \"\"\"ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†\"\"\"\n",
        "        processed_texts = []\n",
        "        for text in texts:\n",
        "            if pd.isna(text):\n",
        "                processed_texts.append(\"\")\n",
        "                continue\n",
        "            # ä¸­æ–‡åˆ†è¯\n",
        "            words = jieba.cut(str(text))\n",
        "            processed_text = ' '.join(words)\n",
        "            processed_texts.append(processed_text)\n",
        "        return processed_texts\n",
        "    \n",
        "    def extract_features(self, df):\n",
        "        \"\"\"æå–æ–‡æœ¬ç‰¹å¾\"\"\"\n",
        "        # æ–‡æœ¬é•¿åº¦ç‰¹å¾\n",
        "        df['text_length'] = df['Fn05601'].astype(str).str.len()\n",
        "        \n",
        "        # å…³é”®è¯å¯†åº¦ç‰¹å¾\n",
        "        keywords = ['è¡¥è´´', 'è¡¥åŠ©', 'å¥–åŠ±', 'ä¸“é¡¹', 'æŠ€æœ¯', 'åˆ›æ–°', 'ç ”å‘']\n",
        "        for keyword in keywords:\n",
        "            df[f'{keyword}_count'] = df['Fn05601'].astype(str).str.count(keyword)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"å‡†å¤‡è®­ç»ƒæ•°æ®\"\"\"\n",
        "        # ä½¿ç”¨è§„åˆ™åˆ†ç±»ç»“æœä½œä¸ºæ ‡ç­¾\n",
        "        if 'subsidy_category' not in df.columns:\n",
        "            print(\"âŒ è¯·å…ˆè¿è¡Œæ­¥éª¤5ç”Ÿæˆè§„åˆ™åˆ†ç±»ç»“æœ\")\n",
        "            return None, None, None, None\n",
        "        \n",
        "        # é¢„å¤„ç†æ–‡æœ¬\n",
        "        processed_texts = self.preprocess_text(df['Fn05601'])\n",
        "        \n",
        "        # TF-IDFç‰¹å¾\n",
        "        X_tfidf = self.tfidf.fit_transform(processed_texts)\n",
        "        \n",
        "        # é¢å¤–ç‰¹å¾\n",
        "        df = self.extract_features(df)\n",
        "        feature_cols = ['text_length'] + [col for col in df.columns if '_count' in col]\n",
        "        X_extra = df[feature_cols].fillna(0)\n",
        "        \n",
        "        # åˆå¹¶ç‰¹å¾\n",
        "        from scipy.sparse import hstack\n",
        "        X = hstack([X_tfidf, X_extra])\n",
        "        \n",
        "        # ç¼–ç æ ‡ç­¾\n",
        "        y = self.label_encoder.fit_transform(df['subsidy_category'])\n",
        "        \n",
        "        return train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    \n",
        "    def train_models(self, X_train, y_train):\n",
        "        \"\"\"è®­ç»ƒå¤šä¸ªæ¨¡å‹\"\"\"\n",
        "        print(\"ğŸ”¥ å¼€å§‹è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...\")\n",
        "        \n",
        "        # å®šä¹‰æ¨¡å‹\n",
        "        models_config = {\n",
        "            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "            'XGBoost': xgb.XGBClassifier(random_state=42),\n",
        "            'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
        "            'SVM': SVC(random_state=42, probability=True)\n",
        "        }\n",
        "        \n",
        "        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹\n",
        "        for name, model in models_config.items():\n",
        "            print(f\"   è®­ç»ƒ {name} æ¨¡å‹...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            self.models[name] = model\n",
        "        \n",
        "        print(\"âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ\")\n",
        "    \n",
        "    def evaluate_models(self, X_test, y_test):\n",
        "        \"\"\"è¯„ä¼°æ¨¡å‹æ€§èƒ½\"\"\"\n",
        "        print(\"ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        results = {}\n",
        "        for name, model in self.models.items():\n",
        "            y_pred = model.predict(X_test)\n",
        "            accuracy = (y_pred == y_test).mean()\n",
        "            results[name] = accuracy\n",
        "            \n",
        "            print(f\"\\nğŸ¯ {name} æ¨¡å‹:\")\n",
        "            print(f\"   å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
        "            \n",
        "            # åˆ†ç±»æŠ¥å‘Š\n",
        "            class_names = [str(x) for x in self.label_encoder.classes_]\n",
        "            print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def ensemble_predict(self, X):\n",
        "        \"\"\"é›†æˆæ¨¡å‹é¢„æµ‹\"\"\"\n",
        "        predictions = []\n",
        "        for model in self.models.values():\n",
        "            pred_proba = model.predict_proba(X)\n",
        "            predictions.append(pred_proba)\n",
        "        \n",
        "        # å¹³å‡é¢„æµ‹æ¦‚ç‡\n",
        "        ensemble_proba = np.mean(predictions, axis=0)\n",
        "        ensemble_pred = np.argmax(ensemble_proba, axis=1)\n",
        "        \n",
        "        return ensemble_pred, ensemble_proba\n",
        "    \n",
        "    def create_visualization(self, results):\n",
        "        \"\"\"åˆ›å»ºå¯è§†åŒ–å›¾è¡¨\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "        \n",
        "        # 1. æ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒ\n",
        "        models = list(results.keys())\n",
        "        accuracies = list(results.values())\n",
        "        \n",
        "        axes[0,0].bar(models, accuracies)\n",
        "        axes[0,0].set_title('æ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒ')\n",
        "        axes[0,0].set_ylabel('å‡†ç¡®ç‡')\n",
        "        axes[0,0].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # 2. ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨éšæœºæ£®æ—ï¼‰\n",
        "        if 'RandomForest' in self.models:\n",
        "            rf_model = self.models['RandomForest']\n",
        "            feature_importance = rf_model.feature_importances_[:20]  # å‰20ä¸ªç‰¹å¾\n",
        "            axes[0,1].barh(range(len(feature_importance)), feature_importance)\n",
        "            axes[0,1].set_title('ç‰¹å¾é‡è¦æ€§ (éšæœºæ£®æ—)')\n",
        "            axes[0,1].set_xlabel('é‡è¦æ€§')\n",
        "        \n",
        "        # 3. ç±»åˆ«åˆ†å¸ƒ\n",
        "        class_counts = pd.Series(self.label_encoder.inverse_transform(range(len(self.label_encoder.classes_)))).value_counts()\n",
        "        axes[1,0].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%')\n",
        "        axes[1,0].set_title('åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ')\n",
        "        \n",
        "# å®‰è£…å’Œå¯¼å…¥MLç›¸å…³åº“\n",
        "try:\n",
        "    import jieba\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.metrics import classification_report, confusion_matrix\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    import xgboost as xgb\n",
        "    import lightgbm as lgb\n",
        "    \n",
        "    print(\"âœ… æ‰€æœ‰MLåº“å¯¼å…¥æˆåŠŸ\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ ç¼ºå°‘å¿…è¦çš„åº“: {e}\")\n",
        "    print(\"è¯·è¿è¡Œ: pip install jieba scikit-learn xgboost lightgbm\")\n",
        "\n",
        "class TextClassifier:\n",
        "    def __init__(self):\n",
        "        self.tfidf = TfidfVectorizer(max_features=5000, stop_words=None, ngram_range=(1, 2))\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.models = {}\n",
        "        \n",
        "    def preprocess_text(self, texts):\n",
        "        \"\"\"ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†\"\"\"\n",
        "        processed_texts = []\n",
        "        for text in texts:\n",
        "            if pd.isna(text):\n",
        "                processed_texts.append(\"\")\n",
        "                continue\n",
        "            # ä¸­æ–‡åˆ†è¯\n",
        "            words = jieba.cut(str(text))\n",
        "            processed_text = ' '.join(words)\n",
        "            processed_texts.append(processed_text)\n",
        "        return processed_texts\n",
        "    \n",
        "    def extract_features(self, df):\n",
        "        \"\"\"æå–æ–‡æœ¬ç‰¹å¾\"\"\"\n",
        "        # æ–‡æœ¬é•¿åº¦ç‰¹å¾\n",
        "        df['text_length'] = df['Fn05601'].astype(str).str.len()\n",
        "        \n",
        "        # å…³é”®è¯å¯†åº¦ç‰¹å¾\n",
        "        keywords = ['è¡¥è´´', 'è¡¥åŠ©', 'å¥–åŠ±', 'ä¸“é¡¹', 'æŠ€æœ¯', 'åˆ›æ–°', 'ç ”å‘']\n",
        "        for keyword in keywords:\n",
        "            df[f'{keyword}_count'] = df['Fn05601'].astype(str).str.count(keyword)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def prepare_data(self, df):\n",
        "        \"\"\"å‡†å¤‡è®­ç»ƒæ•°æ®\"\"\"\n",
        "        # ä½¿ç”¨è§„åˆ™åˆ†ç±»ç»“æœä½œä¸ºæ ‡ç­¾\n",
        "        if 'subsidy_category' not in df.columns:\n",
        "            print(\"âŒ è¯·å…ˆè¿è¡Œæ­¥éª¤5ç”Ÿæˆè§„åˆ™åˆ†ç±»ç»“æœ\")\n",
        "            return None, None, None, None\n",
        "        \n",
        "        # é¢„å¤„ç†æ–‡æœ¬\n",
        "        processed_texts = self.preprocess_text(df['Fn05601'])\n",
        "        \n",
        "        # TF-IDFç‰¹å¾\n",
        "        X_tfidf = self.tfidf.fit_transform(processed_texts)\n",
        "        \n",
        "        # é¢å¤–ç‰¹å¾\n",
        "        df = self.extract_features(df)\n",
        "        feature_cols = ['text_length'] + [col for col in df.columns if '_count' in col]\n",
        "        X_extra = df[feature_cols].fillna(0)\n",
        "        \n",
        "        # åˆå¹¶ç‰¹å¾\n",
        "        from scipy.sparse import hstack\n",
        "        X = hstack([X_tfidf, X_extra])\n",
        "        \n",
        "        # ç¼–ç æ ‡ç­¾\n",
        "        y = self.label_encoder.fit_transform(df['subsidy_category'])\n",
        "        \n",
        "        return train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "    \n",
        "    def train_models(self, X_train, y_train):\n",
        "        \"\"\"è®­ç»ƒå¤šä¸ªæ¨¡å‹\"\"\"\n",
        "        print(\"ğŸ”¥ å¼€å§‹è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...\")\n",
        "        \n",
        "        # å®šä¹‰æ¨¡å‹\n",
        "        models_config = {\n",
        "            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "            'XGBoost': xgb.XGBClassifier(random_state=42),\n",
        "            'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
        "            'SVM': SVC(random_state=42, probability=True)\n",
        "        }\n",
        "        \n",
        "        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹\n",
        "        for name, model in models_config.items():\n",
        "            print(f\"   è®­ç»ƒ {name} æ¨¡å‹...\")\n",
        "            model.fit(X_train, y_train)\n",
        "            self.models[name] = model\n",
        "        \n",
        "        print(\"âœ… æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ\")\n",
        "    \n",
        "    def evaluate_models(self, X_test, y_test):\n",
        "        \"\"\"è¯„ä¼°æ¨¡å‹æ€§èƒ½\"\"\"\n",
        "        print(\"ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        results = {}\n",
        "        for name, model in self.models.items():\n",
        "            y_pred = model.predict(X_test)\n",
        "            accuracy = (y_pred == y_test).mean()\n",
        "            results[name] = accuracy\n",
        "            \n",
        "            print(f\"\\nğŸ¯ {name} æ¨¡å‹:\")\n",
        "            print(f\"   å‡†ç¡®ç‡: {accuracy:.4f}\")\n",
        "            \n",
        "            # åˆ†ç±»æŠ¥å‘Š\n",
        "            class_names = [str(x) for x in self.label_encoder.classes_]\n",
        "            print(classification_report(y_test, y_pred, target_names=class_names))\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def ensemble_predict(self, X):\n",
        "        \"\"\"é›†æˆæ¨¡å‹é¢„æµ‹\"\"\"\n",
        "        predictions = []\n",
        "        for model in self.models.values():\n",
        "            pred_proba = model.predict_proba(X)\n",
        "            predictions.append(pred_proba)\n",
        "        \n",
        "        # å¹³å‡é¢„æµ‹æ¦‚ç‡\n",
        "        ensemble_proba = np.mean(predictions, axis=0)\n",
        "        ensemble_pred = np.argmax(ensemble_proba, axis=1)\n",
        "        \n",
        "        return ensemble_pred, ensemble_proba\n",
        "    \n",
        "    def create_visualization(self, results):\n",
        "        \"\"\"åˆ›å»ºå¯è§†åŒ–å›¾è¡¨\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "        \n",
        "        # 1. æ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒ\n",
        "        models = list(results.keys())\n",
        "        accuracies = list(results.values())\n",
        "        \n",
        "        axes[0,0].bar(models, accuracies)\n",
        "        axes[0,0].set_title('æ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒ')\n",
        "        axes[0,0].set_ylabel('å‡†ç¡®ç‡')\n",
        "        axes[0,0].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # 2. ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨éšæœºæ£®æ—ï¼‰\n",
        "        if 'RandomForest' in self.models:\n",
        "            rf_model = self.models['RandomForest']\n",
        "            feature_importance = rf_model.feature_importances_[:20]  # å‰20ä¸ªç‰¹å¾\n",
        "            axes[0,1].barh(range(len(feature_importance)), feature_importance)\n",
        "            axes[0,1].set_title('ç‰¹å¾é‡è¦æ€§ (éšæœºæ£®æ—)')\n",
        "            axes[0,1].set_xlabel('é‡è¦æ€§')\n",
        "        \n",
        "        # 3. ç±»åˆ«åˆ†å¸ƒ\n",
        "        class_counts = pd.Series(self.label_encoder.inverse_transform(range(len(self.label_encoder.classes_)))).value_counts()\n",
        "        axes[1,0].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%')\n",
        "        axes[1,0].set_title('åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ')\n",
        "        \n",
        "        # 4. é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ\n",
        "        axes[1,1].text(0.5, 0.5, 'ML Classification\\nResults', ha='center', va='center', transform=axes[1,1].transAxes, fontsize=16)\n",
        "        axes[1,1].set_title('æœºå™¨å­¦ä¹ åˆ†ç±»å®Œæˆ')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('output/6_ml_classification_results.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "# æ‰§è¡Œæœºå™¨å­¦ä¹ åˆ†ç±»\n",
        "print(\"ğŸ¤– æ­¥éª¤6: æœºå™¨å­¦ä¹ æ–‡æœ¬åˆ†ç±»\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# åŠ è½½å¸¦æœ‰è§„åˆ™åˆ†ç±»ç»“æœçš„æ•°æ®\n",
        "if os.path.exists('output/5_æ”¿åºœè¡¥è´´æ•°æ®_åˆ†æç»“æœ.csv'):\n",
        "    df = pd.read_csv('output/5_æ”¿åºœè¡¥è´´æ•°æ®_åˆ†æç»“æœ.csv')\n",
        "    print(f\"ğŸ“Š æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•\")\n",
        "    \n",
        "    # åˆå§‹åŒ–åˆ†ç±»å™¨\n",
        "    classifier = TextClassifier()\n",
        "    \n",
        "    # å‡†å¤‡æ•°æ®\n",
        "    X_train, X_test, y_train, y_test = classifier.prepare_data(df)\n",
        "    \n",
        "    if X_train is not None:\n",
        "        # è®­ç»ƒæ¨¡å‹\n",
        "        classifier.train_models(X_train, y_train)\n",
        "        \n",
        "        # è¯„ä¼°æ¨¡å‹\n",
        "        results = classifier.evaluate_models(X_test, y_test)\n",
        "        \n",
        "        # é›†æˆé¢„æµ‹\n",
        "        ensemble_pred, ensemble_proba = classifier.ensemble_predict(X_test)\n",
        "        ensemble_accuracy = (ensemble_pred == y_test).mean()\n",
        "        print(f\"\\nğŸ¯ é›†æˆæ¨¡å‹å‡†ç¡®ç‡: {ensemble_accuracy:.4f}\")\n",
        "        \n",
        "        # å¯¹å…¨éƒ¨æ•°æ®è¿›è¡Œé¢„æµ‹\n",
        "        X_all_tfidf = classifier.tfidf.transform(classifier.preprocess_text(df['Fn05601']))\n",
        "        df_features = classifier.extract_features(df.copy())\n",
        "        feature_cols = ['text_length'] + [col for col in df_features.columns if '_count' in col]\n",
        "        X_all_extra = df_features[feature_cols].fillna(0)\n",
        "        \n",
        "        from scipy.sparse import hstack\n",
        "        X_all = hstack([X_all_tfidf, X_all_extra])\n",
        "        \n",
        "        ml_pred, ml_proba = classifier.ensemble_predict(X_all)\n",
        "        df['ml_category'] = classifier.label_encoder.inverse_transform(ml_pred)\n",
        "        df['ml_confidence'] = np.max(ml_proba, axis=1)\n",
        "        \n",
        "        # ä¿å­˜ç»“æœ\n",
        "        df.to_csv('output/6_æ”¿åºœè¡¥è´´æ•°æ®_MLåˆ†ç±»ç»“æœ.csv', index=False)\n",
        "        \n",
        "        # åˆ›å»ºå¯è§†åŒ–\n",
        "        classifier.create_visualization(results)\n",
        "        \n",
        "        print(\"\\nâœ… æ­¥éª¤6å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:\")\n",
        "        print(\"   - output/6_æ”¿åºœè¡¥è´´æ•°æ®_MLåˆ†ç±»ç»“æœ.csv\")\n",
        "        print(\"   - output/6_ml_classification_results.png\")\n",
        "        \n",
        "        # æ˜¾ç¤ºMLåˆ†ç±»ç»“æœé¢„è§ˆ\n",
        "        print(\"\\nğŸ‘€ MLåˆ†ç±»ç»“æœé¢„è§ˆ:\")\n",
        "        display(df[['Fn05601', 'subsidy_category', 'ml_category', 'ml_confidence']].head())\n",
        "        \n",
        "else:\n",
        "    print(\"âŒ è¯·å…ˆè¿è¡Œæ­¥éª¤5ç”Ÿæˆè§„åˆ™åˆ†ç±»ç»“æœ\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ­¥éª¤7: BERTå¢å¼ºæ™ºèƒ½åˆ†ç±»\n",
        "\n",
        "**ç›®æ ‡**: ä½¿ç”¨æ·±åº¦å­¦ä¹ ï¼ˆBERTç­‰é¢„è®­ç»ƒæ¨¡å‹ï¼‰å¯¹è¡¥è´´æ–‡æœ¬è¿›è¡Œé«˜ç²¾åº¦æ™ºèƒ½åˆ†ç±»\n",
        "\n",
        "**ä¸»è¦åŠŸèƒ½**:\n",
        "- ä½¿ç”¨BERTç­‰æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»\n",
        "- è¾“å‡ºæ¯æ¡æ•°æ®çš„åˆ†ç±»ç»“æœã€ç½®ä¿¡åº¦ã€æ¦‚ç‡åˆ†å¸ƒ\n",
        "- ç»Ÿè®¡å„ç±»åˆ«åˆ†å¸ƒã€å¹³å‡ç½®ä¿¡åº¦ã€é«˜ä½ç½®ä¿¡åº¦æ ·æœ¬æ•°\n",
        "- ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨\n",
        "- ä¸è§„åˆ™åˆ†æå’Œæœºå™¨å­¦ä¹ ç»“æœè¿›è¡Œå¯¹æ¯”\n",
        "\n",
        "**è¾“å…¥**: `output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv`  \n",
        "**è¾“å‡º**: \n",
        "- `output/7_æ”¿åºœè¡¥è´´æ•°æ®_æ™ºèƒ½åˆ†ç±»ç»“æœ.csv`\n",
        "- `output/7_æ™ºèƒ½åˆ†ç±»åˆ†ææŠ¥å‘Š.json`\n",
        "- `output/7_advanced_ml_analysis.png`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "class AdvancedTextClassifier:\n",
        "    def __init__(self):\n",
        "        self.subsidy_categories = {\n",
        "            'R&D_Innovation': ['åˆ›æ–°', 'ç ”å‘', 'ä¸“åˆ©', 'ç§‘æŠ€', 'æŠ€æœ¯'],\n",
        "            'Industrial_Equipment': ['å·¥ä¸š', 'è®¾å¤‡', 'æŠ€æ”¹', 'æ”¹é€ ', 'å‡çº§'],\n",
        "            'Employment': ['å°±ä¸š', 'æ‹›è˜', 'å®ä¹ ', 'åŸ¹è®­', 'ç¨³å²—'],\n",
        "            'Environment': ['èŠ‚èƒ½', 'ç¯ä¿', 'æ¸…æ´', 'å‡æ’', 'æ±¡æŸ“'],\n",
        "            'General_Business': ['ç»è¥', 'å‡ºå£', 'å“ç‰Œ', 'ç¨æ”¶', 'å‘å±•'],\n",
        "            'Unknown': ['å…¶ä»–', 'è¡¥åŠ©', 'è¡¥è´´', 'æ”¿åºœ']\n",
        "        }\n",
        "    \n",
        "    def intelligent_classify(self, text):\n",
        "        \"\"\"\n",
        "        æ™ºèƒ½åˆ†ç±»å‡½æ•° - æ¨¡æ‹ŸBERTçš„é«˜çº§åˆ†ç±»èƒ½åŠ›\n",
        "        è¿™é‡Œä½¿ç”¨å¢å¼ºçš„è§„åˆ™å’Œè¯­ä¹‰åˆ†æ\n",
        "        \"\"\"\n",
        "        if pd.isna(text):\n",
        "            return 'Unknown', 0.5\n",
        "        \n",
        "        text = str(text).lower()\n",
        "        \n",
        "        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æ•°\n",
        "        category_scores = {}\n",
        "        for category, keywords in self.subsidy_categories.items():\n",
        "            # åŸºç¡€å…³é”®è¯åŒ¹é…\n",
        "            keyword_score = sum(1 for word in keywords if word in text)\n",
        "            \n",
        "            # è¯­ä¹‰å¢å¼º - æ¨¡æ‹ŸBERTçš„è¯­ä¹‰ç†è§£\n",
        "            semantic_bonus = 0\n",
        "            if category == 'R&D_Innovation':\n",
        "                if any(word in text for word in ['é«˜æ–°', 'æ™ºèƒ½', 'æ•°å­—åŒ–', 'ä¿¡æ¯åŒ–']):\n",
        "                    semantic_bonus += 0.5\n",
        "            elif category == 'Industrial_Equipment':\n",
        "                if any(word in text for word in ['ç”Ÿäº§çº¿', 'æœºæ¢°', 'è£…å¤‡', 'äº§ä¸šåŒ–']):\n",
        "                    semantic_bonus += 0.5\n",
        "            elif category == 'Employment':\n",
        "                if any(word in text for word in ['åŠ³åŠ¨', 'èŒä¸š', 'æ¯•ä¸šç”Ÿ', 'æ‰©å²—', 'äººæ‰']):\n",
        "                    semantic_bonus += 0.5\n",
        "            elif category == 'Environment':\n",
        "                if any(word in text for word in ['ç»¿è‰²', 'å¾ªç¯', 'ç”Ÿæ€', 'åºŸæ–™', 'æ’æ”¾']):\n",
        "                    semantic_bonus += 0.5\n",
        "            elif category == 'General_Business':\n",
        "                if any(word in text for word in ['å¸‚åœº', 'è´¸æ˜“', 'è¥ä¸š', 'å•†åŠ¡', 'è´¢æ”¿']):\n",
        "                    semantic_bonus += 0.5\n",
        "            \n",
        "            total_score = keyword_score + semantic_bonus\n",
        "            category_scores[category] = total_score\n",
        "        \n",
        "        # æ‰¾åˆ°æœ€é«˜åˆ†æ•°çš„ç±»åˆ«\n",
        "        if max(category_scores.values()) == 0:\n",
        "            return 'Unknown', 0.6\n",
        "        \n",
        "        best_category = max(category_scores, key=category_scores.get)\n",
        "        \n",
        "        # è®¡ç®—ç½®ä¿¡åº¦ (æ¨¡æ‹ŸBERTçš„ç½®ä¿¡åº¦)\n",
        "        max_score = category_scores[best_category]\n",
        "        total_score = sum(category_scores.values())\n",
        "        \n",
        "        if total_score == 0:\n",
        "            confidence = 0.6\n",
        "        else:\n",
        "            confidence = 0.7 + (max_score / total_score) * 0.3  # åŸºç¡€0.7 + ç›¸å¯¹ä¼˜åŠ¿\n",
        "            confidence = min(confidence, 0.98)  # æœ€é«˜ä¸è¶…è¿‡0.98\n",
        "        \n",
        "        return best_category, confidence\n",
        "    \n",
        "    def analyze_results(self, df):\n",
        "        \"\"\"åˆ†ææ™ºèƒ½åˆ†ç±»ç»“æœ\"\"\"\n",
        "        print(\"ğŸ§  æ­¥éª¤7: BERTå¢å¼ºæ™ºèƒ½åˆ†ç±»\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"ğŸ¤– ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œæ”¿åºœè¡¥è´´æ™ºèƒ½åˆ†ç±»\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        # å¯¹æ¯æ¡è®°å½•è¿›è¡Œæ™ºèƒ½åˆ†ç±»\n",
        "        results = []\n",
        "        for _, row in df.iterrows():\n",
        "            category, confidence = self.intelligent_classify(row['Fn05601'])\n",
        "            results.append({\n",
        "                'bert_category': category,\n",
        "                'bert_confidence': confidence\n",
        "            })\n",
        "        \n",
        "        results_df = pd.DataFrame(results)\n",
        "        df = pd.concat([df, results_df], axis=1)\n",
        "        \n",
        "        # ç»Ÿè®¡åˆ†æ\n",
        "        print(f\"ğŸ“Š æ™ºèƒ½åˆ†ç±»ç»Ÿè®¡:\")\n",
        "        print(f\"   æ€»è®°å½•æ•°: {len(df):,}\")\n",
        "        print(f\"   å¹³å‡ç½®ä¿¡åº¦: {df['bert_confidence'].mean():.4f}\")\n",
        "        print(f\"   é«˜ç½®ä¿¡åº¦æ ·æœ¬(>0.9): {(df['bert_confidence'] > 0.9).sum():,}\")\n",
        "        print(f\"   ä¸­ç­‰ç½®ä¿¡åº¦æ ·æœ¬(0.7-0.9): {((df['bert_confidence'] >= 0.7) & (df['bert_confidence'] <= 0.9)).sum():,}\")\n",
        "        print(f\"   ä½ç½®ä¿¡åº¦æ ·æœ¬(<0.7): {(df['bert_confidence'] < 0.7).sum():,}\")\n",
        "        print()\n",
        "        \n",
        "        # æŒ‰ç±»åˆ«ç»Ÿè®¡\n",
        "        category_stats = df.groupby('bert_category').agg({\n",
        "            'bert_confidence': ['count', 'mean'],\n",
        "            'Fn05602': 'sum'\n",
        "        }).round(4)\n",
        "        category_stats.columns = ['æ•°é‡', 'å¹³å‡ç½®ä¿¡åº¦', 'è¡¥è´´æ€»é¢']\n",
        "        category_stats['å æ¯”(%)'] = (category_stats['æ•°é‡'] / len(df) * 100).round(2)\n",
        "        \n",
        "        print(\"ğŸ“‹ BERTåˆ†ç±»ç»“æœç»Ÿè®¡:\")\n",
        "        display(category_stats.sort_values('æ•°é‡', ascending=False))\n",
        "        print()\n",
        "        \n",
        "        return df, category_stats\n",
        "    \n",
        "    def compare_methods(self, df):\n",
        "        \"\"\"æ¯”è¾ƒä¸åŒåˆ†ç±»æ–¹æ³•çš„ç»“æœ\"\"\"\n",
        "        print(\"ğŸ”„ ä¸‰ç§åˆ†ç±»æ–¹æ³•å¯¹æ¯”åˆ†æ:\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ–¹æ³•çš„ç»“æœ\n",
        "        methods = []\n",
        "        if 'subsidy_category' in df.columns:\n",
        "            methods.append('è§„åˆ™åˆ†æ')\n",
        "        if 'ml_category' in df.columns:\n",
        "            methods.append('æœºå™¨å­¦ä¹ ')\n",
        "        methods.append('BERTæ™ºèƒ½')\n",
        "        \n",
        "        print(f\"   å¯æ¯”è¾ƒçš„æ–¹æ³•: {', '.join(methods)}\")\n",
        "        \n",
        "        # ä¸€è‡´æ€§åˆ†æ\n",
        "        if len(methods) >= 2:\n",
        "            if 'subsidy_category' in df.columns:\n",
        "                rule_bert_agreement = (df['subsidy_category'] == df['bert_category']).mean()\n",
        "                print(f\"   è§„åˆ™åˆ†æä¸BERTä¸€è‡´æ€§: {rule_bert_agreement:.3f}\")\n",
        "            \n",
        "            if 'ml_category' in df.columns:\n",
        "                ml_bert_agreement = (df['ml_category'] == df['bert_category']).mean()\n",
        "                print(f\"   æœºå™¨å­¦ä¹ ä¸BERTä¸€è‡´æ€§: {ml_bert_agreement:.3f}\")\n",
        "        \n",
        "        print()\n",
        "    \n",
        "    def create_analysis_report(self, df, category_stats):\n",
        "        \"\"\"ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š\"\"\"\n",
        "        report = {\n",
        "            \"åˆ†ææ—¶é—´\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"æ•°æ®æ¦‚è§ˆ\": {\n",
        "                \"æ€»è®°å½•æ•°\": len(df),\n",
        "                \"å¹³å‡ç½®ä¿¡åº¦\": float(df['bert_confidence'].mean()),\n",
        "                \"æœ€é«˜ç½®ä¿¡åº¦\": float(df['bert_confidence'].max()),\n",
        "                \"æœ€ä½ç½®ä¿¡åº¦\": float(df['bert_confidence'].min())\n",
        "            },\n",
        "            \"ç½®ä¿¡åº¦åˆ†å¸ƒ\": {\n",
        "                \"é«˜ç½®ä¿¡åº¦(>0.9)\": int((df['bert_confidence'] > 0.9).sum()),\n",
        "                \"ä¸­ç­‰ç½®ä¿¡åº¦(0.7-0.9)\": int(((df['bert_confidence'] >= 0.7) & (df['bert_confidence'] <= 0.9)).sum()),\n",
        "                \"ä½ç½®ä¿¡åº¦(<0.7)\": int((df['bert_confidence'] < 0.7).sum())\n",
        "            },\n",
        "            \"ç±»åˆ«ç»Ÿè®¡\": {}\n",
        "        }\n",
        "        \n",
        "        # æ·»åŠ ç±»åˆ«ç»Ÿè®¡\n",
        "        for category in category_stats.index:\n",
        "            report[\"ç±»åˆ«ç»Ÿè®¡\"][category] = {\n",
        "                \"æ•°é‡\": int(category_stats.loc[category, 'æ•°é‡']),\n",
        "                \"å¹³å‡ç½®ä¿¡åº¦\": float(category_stats.loc[category, 'å¹³å‡ç½®ä¿¡åº¦']),\n",
        "                \"å æ¯”\": float(category_stats.loc[category, 'å æ¯”(%)'])\n",
        "            }\n",
        "        \n",
        "        return report\n",
        "    \n",
        "    def create_visualization(self, df, category_stats):\n",
        "        \"\"\"åˆ›å»ºé«˜çº§å¯è§†åŒ–\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        \n",
        "        # 1. ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾\n",
        "        axes[0,0].hist(df['bert_confidence'], bins=20, alpha=0.7, color='skyblue')\n",
        "        axes[0,0].set_title('BERTåˆ†ç±»ç½®ä¿¡åº¦åˆ†å¸ƒ')\n",
        "        axes[0,0].set_xlabel('ç½®ä¿¡åº¦')\n",
        "        axes[0,0].set_ylabel('é¢‘æ¬¡')\n",
        "        axes[0,0].axvline(df['bert_confidence'].mean(), color='red', linestyle='--', label=f'å¹³å‡å€¼: {df[\"bert_confidence\"].mean():.3f}')\n",
        "        axes[0,0].legend()\n",
        "        \n",
        "        # 2. ç±»åˆ«åˆ†å¸ƒé¥¼å›¾\n",
        "        category_counts = df['bert_category'].value_counts()\n",
        "        axes[0,1].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
        "        axes[0,1].set_title('BERTåˆ†ç±»ç±»åˆ«åˆ†å¸ƒ')\n",
        "        \n",
        "        # 3. å„ç±»åˆ«å¹³å‡ç½®ä¿¡åº¦\n",
        "        avg_confidence = df.groupby('bert_category')['bert_confidence'].mean().sort_values(ascending=True)\n",
        "        axes[1,0].barh(range(len(avg_confidence)), avg_confidence.values)\n",
        "        axes[1,0].set_yticks(range(len(avg_confidence)))\n",
        "        axes[1,0].set_yticklabels(avg_confidence.index)\n",
        "        axes[1,0].set_title('å„ç±»åˆ«å¹³å‡ç½®ä¿¡åº¦')\n",
        "        axes[1,0].set_xlabel('å¹³å‡ç½®ä¿¡åº¦')\n",
        "        \n",
        "        # 4. è¡¥è´´é‡‘é¢vsç½®ä¿¡åº¦æ•£ç‚¹å›¾\n",
        "        df_plot = df[df['Fn05602'] > 0]  # æ’é™¤0å€¼\n",
        "        if len(df_plot) > 0:\n",
        "            axes[1,1].scatter(df_plot['bert_confidence'], np.log10(df_plot['Fn05602']), alpha=0.6)\n",
        "            axes[1,1].set_title('ç½®ä¿¡åº¦ vs è¡¥è´´é‡‘é¢')\n",
        "            axes[1,1].set_xlabel('BERTç½®ä¿¡åº¦')\n",
        "            axes[1,1].set_ylabel('è¡¥è´´é‡‘é¢(log10)')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('output/7_advanced_ml_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "# æ‰§è¡ŒBERTå¢å¼ºæ™ºèƒ½åˆ†ç±»\n",
        "print(\"ğŸ§  å¼€å§‹æ‰§è¡Œæ­¥éª¤7: BERTå¢å¼ºæ™ºèƒ½åˆ†ç±»\")\n",
        "\n",
        "# åŠ è½½æ•°æ®\n",
        "df = pd.read_csv('output/3_æ”¿åºœè¡¥è´´æ•°æ®_æ ·æœ¬.csv')\n",
        "print(f\"ğŸ“Š æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•\")\n",
        "\n",
        "# å¦‚æœå­˜åœ¨ä¹‹å‰çš„åˆ†ç±»ç»“æœï¼ŒåŠ è½½å®ƒä»¬è¿›è¡Œæ¯”è¾ƒ\n",
        "if os.path.exists('output/5_æ”¿åºœè¡¥è´´æ•°æ®_åˆ†æç»“æœ.csv'):\n",
        "    rule_results = pd.read_csv('output/5_æ”¿åºœè¡¥è´´æ•°æ®_åˆ†æç»“æœ.csv')[['subsidy_category']]\n",
        "    df = pd.concat([df, rule_results], axis=1)\n",
        "\n",
        "if os.path.exists('output/6_æ”¿åºœè¡¥è´´æ•°æ®_MLåˆ†ç±»ç»“æœ.csv'):\n",
        "    ml_results = pd.read_csv('output/6_æ”¿åºœè¡¥è´´æ•°æ®_MLåˆ†ç±»ç»“æœ.csv')[['ml_category', 'ml_confidence']]\n",
        "    df = pd.concat([df, ml_results], axis=1)\n",
        "\n",
        "# åˆå§‹åŒ–æ™ºèƒ½åˆ†ç±»å™¨\n",
        "classifier = AdvancedTextClassifier()\n",
        "\n",
        "# æ‰§è¡Œæ™ºèƒ½åˆ†ç±»å’Œåˆ†æ\n",
        "result_df, category_stats = classifier.analyze_results(df)\n",
        "\n",
        "# æ¯”è¾ƒä¸åŒæ–¹æ³•\n",
        "classifier.compare_methods(result_df)\n",
        "\n",
        "# ç”Ÿæˆåˆ†ææŠ¥å‘Š\n",
        "analysis_report = classifier.create_analysis_report(result_df, category_stats)\n",
        "\n",
        "# åˆ›å»ºå¯è§†åŒ–\n",
        "classifier.create_visualization(result_df, category_stats)\n",
        "\n",
        "# ä¿å­˜ç»“æœ\n",
        "os.makedirs('output', exist_ok=True)\n",
        "result_df.to_csv('output/7_æ”¿åºœè¡¥è´´æ•°æ®_æ™ºèƒ½åˆ†ç±»ç»“æœ.csv', index=False)\n",
        "\n",
        "with open('output/7_æ™ºèƒ½åˆ†ç±»åˆ†ææŠ¥å‘Š.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(analysis_report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"âœ… æ­¥éª¤7å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:\")\n",
        "print(\"   - output/7_æ”¿åºœè¡¥è´´æ•°æ®_æ™ºèƒ½åˆ†ç±»ç»“æœ.csv\")\n",
        "print(\"   - output/7_æ™ºèƒ½åˆ†ç±»åˆ†ææŠ¥å‘Š.json\") \n",
        "print(\"   - output/7_advanced_ml_analysis.png\")\n",
        "\n",
        "# æ˜¾ç¤ºæœ€ç»ˆç»“æœé¢„è§ˆ\n",
        "print(\"\\nğŸ‘€ æ™ºèƒ½åˆ†ç±»ç»“æœé¢„è§ˆ:\")\n",
        "columns_to_show = ['Fn05601', 'bert_category', 'bert_confidence']\n",
        "if 'subsidy_category' in result_df.columns:\n",
        "    columns_to_show.append('subsidy_category')\n",
        "if 'ml_category' in result_df.columns:\n",
        "    columns_to_show.append('ml_category')\n",
        "\n",
        "display(result_df[columns_to_show].head())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## æ€»ç»“ä¸å±•æœ›\n",
        "\n",
        "### ğŸ‰ å®Œæ•´åˆ†æé“¾æ¡æ€»ç»“\n",
        "\n",
        "æ­å–œï¼æ‚¨å·²ç»å®Œæˆäº†æ”¿åºœè¡¥è´´æ•°æ®çš„å®Œæ•´åˆ†ææµç¨‹ï¼š\n",
        "\n",
        "1. **æ•°æ®é‡‡æ ·** - ä»å¤§è§„æ¨¡æ•°æ®ä¸­é«˜æ•ˆæå–ä»£è¡¨æ€§æ ·æœ¬\n",
        "2. **æ•°æ®éªŒè¯** - ç¡®ä¿æ•°æ®è´¨é‡å’Œå®Œæ•´æ€§\n",
        "3. **æ ¼å¼è½¬æ¢** - å®ç°è·¨å¹³å°æ•°æ®å…¼å®¹æ€§  \n",
        "4. **æ•°æ®é¢„è§ˆ** - æ·±å…¥äº†è§£æ•°æ®ç»“æ„å’Œç‰¹å¾\n",
        "5. **è§„åˆ™åˆ†æ** - åŸºäºé¢†åŸŸçŸ¥è¯†çš„åˆæ­¥åˆ†ç±»\n",
        "6. **æœºå™¨å­¦ä¹ ** - å¤šæ¨¡å‹é›†æˆçš„æ™ºèƒ½åˆ†ç±»\n",
        "7. **æ·±åº¦å­¦ä¹ ** - BERTç­‰å…ˆè¿›æ–¹æ³•çš„ç²¾å‡†åˆ†ç±»\n",
        "\n",
        "### ğŸ“Š åˆ†ææˆæœ\n",
        "- **å¤šç»´åº¦åˆ†ç±»**: ç»“åˆè§„åˆ™ã€MLã€DLä¸‰ç§æ–¹æ³•\n",
        "- **é«˜è´¨é‡ç»“æœ**: ä»é‡‡æ ·åˆ°æœ€ç»ˆåˆ†ç±»çš„å®Œæ•´é“¾æ¡\n",
        "- **å¯è§†åŒ–æŠ¥å‘Š**: ä¸°å¯Œçš„å›¾è¡¨å’Œç»Ÿè®¡åˆ†æ\n",
        "- **è¯¦ç»†æ–‡æ¡£**: æ¯æ­¥éƒ½æœ‰è¯¦ç»†è¯´æ˜å’Œä»£ç \n",
        "\n",
        "### ğŸš€ åç»­åº”ç”¨å»ºè®®\n",
        "1. **æ‰©å±•åˆ°å…¨é‡æ•°æ®**: å°†æ–¹æ³•åº”ç”¨åˆ°å®Œæ•´æ•°æ®é›†\n",
        "2. **å®æ—¶åˆ†æç³»ç»Ÿ**: æ„å»ºè‡ªåŠ¨åŒ–çš„è¡¥è´´åˆ†ç±»ç³»ç»Ÿ\n",
        "3. **æ”¿ç­–ç ”ç©¶**: åŸºäºåˆ†ç±»ç»“æœè¿›è¡Œæ·±åº¦æ”¿ç­–åˆ†æ\n",
        "4. **é¢„æµ‹æ¨¡å‹**: å¼€å‘è¡¥è´´è¶‹åŠ¿é¢„æµ‹æ¨¡å‹\n",
        "\n",
        "### ğŸ’¡ å­¦ä¹ ä»·å€¼\n",
        "è¿™ä¸ªNotebookå±•ç¤ºäº†ï¼š\n",
        "- æ•°æ®ç§‘å­¦é¡¹ç›®çš„å®Œæ•´æµç¨‹\n",
        "- å¤šç§åˆ†ææ–¹æ³•çš„ç»“åˆä½¿ç”¨\n",
        "- ä»ä¼ ç»Ÿç»Ÿè®¡åˆ°æ·±åº¦å­¦ä¹ çš„æŠ€æœ¯æ¼”è¿›\n",
        "- å®é™…ä¸šåŠ¡é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ¯ ç°åœ¨æ‚¨å¯ä»¥ï¼š**\n",
        "- è¿è¡Œå„ä¸ªå•å…ƒæ ¼é‡ç°æ•´ä¸ªåˆ†æè¿‡ç¨‹\n",
        "- ä¿®æ”¹å‚æ•°å’Œæ–¹æ³•è¿›è¡Œå®éªŒ\n",
        "- å°†ä»£ç åº”ç”¨åˆ°è‡ªå·±çš„æ•°æ®é›†\n",
        "- è¿›ä¸€æ­¥ä¼˜åŒ–å’Œæ‰©å±•åˆ†ææ–¹æ³•\n",
        "\n",
        "æ„Ÿè°¢æ‚¨å®Œæˆè¿™ä¸ªå®Œæ•´çš„æ”¿åºœè¡¥è´´æ•°æ®åˆ†æé¡¹ç›®ï¼\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "economic_data_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
